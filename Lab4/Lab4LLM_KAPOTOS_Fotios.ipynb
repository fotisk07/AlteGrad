{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5e01754b68774d39a032a446c28d4b7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c6f5ecd44927430eb2b3c3fc634813b5",
              "IPY_MODEL_f07de4d28dc6489199a1268fa8ccf9db",
              "IPY_MODEL_61b91a5a1a1d4f83af16a58d12ba68fe"
            ],
            "layout": "IPY_MODEL_694edbddbdf14af3904d1b339041612e"
          }
        },
        "c6f5ecd44927430eb2b3c3fc634813b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e9dc6a222560461daea7826b4cb23022",
            "placeholder": "​",
            "style": "IPY_MODEL_a7fcf159596744879a8d6a461a909335",
            "value": "config.json: 100%"
          }
        },
        "f07de4d28dc6489199a1268fa8ccf9db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a3ba64217d514de0ba32182797c4186f",
            "max": 841,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7fe70a9b21cf489d8e4c90c958a10ac6",
            "value": 841
          }
        },
        "61b91a5a1a1d4f83af16a58d12ba68fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_19be444790644ffbbe1fe9ad59444192",
            "placeholder": "​",
            "style": "IPY_MODEL_cbffe446e191466fa64cb328906c0d68",
            "value": " 841/841 [00:00&lt;00:00, 84.0kB/s]"
          }
        },
        "694edbddbdf14af3904d1b339041612e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e9dc6a222560461daea7826b4cb23022": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a7fcf159596744879a8d6a461a909335": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a3ba64217d514de0ba32182797c4186f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7fe70a9b21cf489d8e4c90c958a10ac6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "19be444790644ffbbe1fe9ad59444192": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cbffe446e191466fa64cb328906c0d68": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0de57ed1bda847a892fabdb65c528064": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d08e28b6b84c44878ffd956355453f21",
              "IPY_MODEL_4372820dee824f888ea16cfb45c834fa",
              "IPY_MODEL_5ab6fbf0ec1e48cab6b8112d33353b68"
            ],
            "layout": "IPY_MODEL_15b7221fa0fb425c9c6dcb9940710878"
          }
        },
        "d08e28b6b84c44878ffd956355453f21": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8655099ae10f40ce9093b256d67cde92",
            "placeholder": "​",
            "style": "IPY_MODEL_422923bd82844a7c9dade9e89c8abca1",
            "value": "model.safetensors.index.json: "
          }
        },
        "4372820dee824f888ea16cfb45c834fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9eec461994b544c6874bc8459c581380",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6b9c25526ade40ef9dd8b91f3ef7e966",
            "value": 1
          }
        },
        "5ab6fbf0ec1e48cab6b8112d33353b68": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c7a1e081e7fe4a0bb1f1f2eaf096596f",
            "placeholder": "​",
            "style": "IPY_MODEL_bd6cdeed57664ddd95116b5e5cf34f14",
            "value": " 62.7k/? [00:00&lt;00:00, 5.09MB/s]"
          }
        },
        "15b7221fa0fb425c9c6dcb9940710878": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8655099ae10f40ce9093b256d67cde92": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "422923bd82844a7c9dade9e89c8abca1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9eec461994b544c6874bc8459c581380": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "6b9c25526ade40ef9dd8b91f3ef7e966": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c7a1e081e7fe4a0bb1f1f2eaf096596f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bd6cdeed57664ddd95116b5e5cf34f14": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "74896ab5c01b4689a024532a16881b7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b63928852f744e0faca469155e4bd66b",
              "IPY_MODEL_c25dd4f054fc42a98cf3f755df1b9545",
              "IPY_MODEL_d5e2e1dc7c1d4b408f8fe421fd959064"
            ],
            "layout": "IPY_MODEL_4a42d562805e483084b2e7ea2686fde6"
          }
        },
        "b63928852f744e0faca469155e4bd66b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cc2701e1d20f4129b992597e4fd8e716",
            "placeholder": "​",
            "style": "IPY_MODEL_961932df04c04dbea6a0d40a6d4d079e",
            "value": "Parse safetensors files: 100%"
          }
        },
        "c25dd4f054fc42a98cf3f755df1b9545": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_126bb8584b8f4b82914bc7858b2eb056",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ff5311cac39341f38b273f5bcb47cd0c",
            "value": 2
          }
        },
        "d5e2e1dc7c1d4b408f8fe421fd959064": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4149cbc267804bf0864a05f982a3ff85",
            "placeholder": "​",
            "style": "IPY_MODEL_9eecf82fbf4949caa22e72965805b14d",
            "value": " 2/2 [00:00&lt;00:00,  5.94it/s]"
          }
        },
        "4a42d562805e483084b2e7ea2686fde6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cc2701e1d20f4129b992597e4fd8e716": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "961932df04c04dbea6a0d40a6d4d079e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "126bb8584b8f4b82914bc7858b2eb056": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ff5311cac39341f38b273f5bcb47cd0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4149cbc267804bf0864a05f982a3ff85": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9eecf82fbf4949caa22e72965805b14d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f4fee765c40d4a5cbabd7dac51d2b2ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f783642b9e6a4626b03a33c2f74aad14",
              "IPY_MODEL_fb5afdebcbc14c00b02f32e1f273dce5",
              "IPY_MODEL_f5dd2fd8dd394e05be68d0c319fe5620"
            ],
            "layout": "IPY_MODEL_8c654dc0d51745b49797ec390e9c469b"
          }
        },
        "f783642b9e6a4626b03a33c2f74aad14": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7b6bca6b987247eab428f6f6045b94fc",
            "placeholder": "​",
            "style": "IPY_MODEL_285494b48f524922b3f66ef79edb05b7",
            "value": "tokenizer_config.json: "
          }
        },
        "fb5afdebcbc14c00b02f32e1f273dce5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4151b17dfcee40ac96e6ab141eecd83b",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c6d4b60ad433409fae97eb2b20513041",
            "value": 1
          }
        },
        "f5dd2fd8dd394e05be68d0c319fe5620": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_476ac114ab384b379ffc4f68f6f6b114",
            "placeholder": "​",
            "style": "IPY_MODEL_48954d618b9c45cfa13bd814ddd7f14b",
            "value": " 7.30k/? [00:00&lt;00:00, 670kB/s]"
          }
        },
        "8c654dc0d51745b49797ec390e9c469b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7b6bca6b987247eab428f6f6045b94fc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "285494b48f524922b3f66ef79edb05b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4151b17dfcee40ac96e6ab141eecd83b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "c6d4b60ad433409fae97eb2b20513041": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "476ac114ab384b379ffc4f68f6f6b114": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "48954d618b9c45cfa13bd814ddd7f14b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "53b6c802bbb542eeb0957aa6eb74be87": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_372221658ab44128b36985f4c43a550d",
              "IPY_MODEL_82b488ce350e47589d5fe9a36758c75a",
              "IPY_MODEL_3192a6cb3548403db2ce3edddf558abf"
            ],
            "layout": "IPY_MODEL_f5909b04a4d045ad9639fd8bdc719ff4"
          }
        },
        "372221658ab44128b36985f4c43a550d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_95c46ec5158d47b6a44592ac61f9006d",
            "placeholder": "​",
            "style": "IPY_MODEL_3418e56a3ffd4a08ac1a3506f37aea66",
            "value": "vocab.json: "
          }
        },
        "82b488ce350e47589d5fe9a36758c75a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ba9810f8af5344bd896359810bcd5776",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b2a7efc9202d4673b1cd8efcd84ae393",
            "value": 1
          }
        },
        "3192a6cb3548403db2ce3edddf558abf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_92b35b457e654b65bdcfa24949b953bd",
            "placeholder": "​",
            "style": "IPY_MODEL_02c534b2971e47be84eff5582dc4b618",
            "value": " 2.78M/? [00:00&lt;00:00, 54.0MB/s]"
          }
        },
        "f5909b04a4d045ad9639fd8bdc719ff4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "95c46ec5158d47b6a44592ac61f9006d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3418e56a3ffd4a08ac1a3506f37aea66": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ba9810f8af5344bd896359810bcd5776": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "b2a7efc9202d4673b1cd8efcd84ae393": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "92b35b457e654b65bdcfa24949b953bd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "02c534b2971e47be84eff5582dc4b618": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1776ae04b1054a28bdbf127f0f5909ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b74e93cd86b14e1d94cae0a098eea926",
              "IPY_MODEL_e59314c4fc764bff9254409515823ad2",
              "IPY_MODEL_4c76703ed579486db071b66dced7b1b7"
            ],
            "layout": "IPY_MODEL_2f43a2aaca3a4c0db072e2e960aad0f0"
          }
        },
        "b74e93cd86b14e1d94cae0a098eea926": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0be140b01eba4cebb8a53b06812dfcdd",
            "placeholder": "​",
            "style": "IPY_MODEL_c2805fdb6274462db996a369b690b782",
            "value": "merges.txt: "
          }
        },
        "e59314c4fc764bff9254409515823ad2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c3612350bb9b4ad0a276830d08e91e69",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1c29731c9b4641ae93065f0219e0912d",
            "value": 1
          }
        },
        "4c76703ed579486db071b66dced7b1b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_081f31432b264747ba626f88e3484cce",
            "placeholder": "​",
            "style": "IPY_MODEL_5cf27cb6a48e4a13ba6acdc191e56a10",
            "value": " 1.67M/? [00:00&lt;00:00, 67.2MB/s]"
          }
        },
        "2f43a2aaca3a4c0db072e2e960aad0f0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0be140b01eba4cebb8a53b06812dfcdd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c2805fdb6274462db996a369b690b782": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c3612350bb9b4ad0a276830d08e91e69": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "1c29731c9b4641ae93065f0219e0912d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "081f31432b264747ba626f88e3484cce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5cf27cb6a48e4a13ba6acdc191e56a10": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "37ff59765e884f3a8f8e7a2f93f4500a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c138619988cc48998bcafb833f7e794c",
              "IPY_MODEL_dc66bb6d03b645e49b6cede613ed2cfc",
              "IPY_MODEL_b5c4886221024758a8900e13fd5aac28"
            ],
            "layout": "IPY_MODEL_f3042e9cce1047979a184588387edb21"
          }
        },
        "c138619988cc48998bcafb833f7e794c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_94c68d59112840829fd9249dd3740244",
            "placeholder": "​",
            "style": "IPY_MODEL_cfde17b2cdf44ec49ffccfa46e0e14be",
            "value": "tokenizer.json: "
          }
        },
        "dc66bb6d03b645e49b6cede613ed2cfc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c578dc3931c048578bf3406da39fdd0f",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_563f635fa25542c496e4636d9e565bc3",
            "value": 1
          }
        },
        "b5c4886221024758a8900e13fd5aac28": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_59236ba3fff3477ebe4b7cc0da030d86",
            "placeholder": "​",
            "style": "IPY_MODEL_dbea4ec5d3454251bd5140899efc7da2",
            "value": " 7.03M/? [00:00&lt;00:00, 105MB/s]"
          }
        },
        "f3042e9cce1047979a184588387edb21": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "94c68d59112840829fd9249dd3740244": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cfde17b2cdf44ec49ffccfa46e0e14be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c578dc3931c048578bf3406da39fdd0f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "563f635fa25542c496e4636d9e565bc3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "59236ba3fff3477ebe4b7cc0da030d86": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dbea4ec5d3454251bd5140899efc7da2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "56e7bfa86a5e47b1ae2b2121f57fd05b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b633c6e31a7e45f9ae0957330d356224",
              "IPY_MODEL_71103d7898d5416f855264696bbb1619",
              "IPY_MODEL_db22159543114f5d8cb06e55c894c83a"
            ],
            "layout": "IPY_MODEL_aca7d4b11dc0430fadee21c77351948a"
          }
        },
        "b633c6e31a7e45f9ae0957330d356224": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_808c52b9b4454cacbafa1dfc4c2b7df6",
            "placeholder": "​",
            "style": "IPY_MODEL_68da55a122304fcb844bc788bebcccd4",
            "value": "generation_config.json: 100%"
          }
        },
        "71103d7898d5416f855264696bbb1619": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_11b825a9698c44a89811d5a929013de8",
            "max": 243,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_901b5c93ce5745129f7803ef891f9bed",
            "value": 243
          }
        },
        "db22159543114f5d8cb06e55c894c83a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_82d342dffefa4532b75bb4208dd0c1e2",
            "placeholder": "​",
            "style": "IPY_MODEL_2676b3eca4844717902cd3396775ec05",
            "value": " 243/243 [00:00&lt;00:00, 22.4kB/s]"
          }
        },
        "aca7d4b11dc0430fadee21c77351948a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "808c52b9b4454cacbafa1dfc4c2b7df6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "68da55a122304fcb844bc788bebcccd4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "11b825a9698c44a89811d5a929013de8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "901b5c93ce5745129f7803ef891f9bed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "82d342dffefa4532b75bb4208dd0c1e2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2676b3eca4844717902cd3396775ec05": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1024a6569b624a49bd522a8686fbd997": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_45ef0d01cc0b467591eba9f721a85b9d",
              "IPY_MODEL_c2a6037670c74dc4bc8f5acfecb6d02e",
              "IPY_MODEL_c2916e9b515547cda2c98f435084a5cf"
            ],
            "layout": "IPY_MODEL_a217bc7d74f842cda2724a2afd08902f"
          }
        },
        "45ef0d01cc0b467591eba9f721a85b9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ce1f07f45e974052be06808eaead3417",
            "placeholder": "​",
            "style": "IPY_MODEL_417e6ab32e1644148a3dd18045c44b90",
            "value": ""
          }
        },
        "c2a6037670c74dc4bc8f5acfecb6d02e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_867b0130c4d940d89c9d4a100f67ba25",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4a209a9026be4eaba5dbec31b7b98f02",
            "value": 0
          }
        },
        "c2916e9b515547cda2c98f435084a5cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d896bd9ba1354c939fe45679cb5de811",
            "placeholder": "​",
            "style": "IPY_MODEL_9138192c7f1c49d69d2b0fda83018e10",
            "value": " 0/? [00:00&lt;?, ?it/s]"
          }
        },
        "a217bc7d74f842cda2724a2afd08902f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ce1f07f45e974052be06808eaead3417": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "417e6ab32e1644148a3dd18045c44b90": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "867b0130c4d940d89c9d4a100f67ba25": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "4a209a9026be4eaba5dbec31b7b98f02": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d896bd9ba1354c939fe45679cb5de811": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9138192c7f1c49d69d2b0fda83018e10": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e2a4c17125b04ff68dd090e1e4941c4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_25b32d47d9f640ab8a081ff522d10832",
              "IPY_MODEL_b22088c27fb846a582161a2fec0fb079",
              "IPY_MODEL_b58733d3bf80457a934832ee93ae4d6b"
            ],
            "layout": "IPY_MODEL_4775c8ce43364700ae48ddf17d885a94"
          }
        },
        "25b32d47d9f640ab8a081ff522d10832": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5937f6cc637f479e8e47c4818de3dea1",
            "placeholder": "​",
            "style": "IPY_MODEL_e5aefffba53b4f80bead239161e5a276",
            "value": ""
          }
        },
        "b22088c27fb846a582161a2fec0fb079": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_da65aaaa04064d51acaa1b237fca78d1",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bafbc9b98ccb426eaa0ebade9a17907b",
            "value": 0
          }
        },
        "b58733d3bf80457a934832ee93ae4d6b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2ae31d8d5e544361a400e265376f667f",
            "placeholder": "​",
            "style": "IPY_MODEL_628ff8e7a6ef4e5f8259bc6293230e82",
            "value": " 0/0 [00:00&lt;?, ?it/s]"
          }
        },
        "4775c8ce43364700ae48ddf17d885a94": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5937f6cc637f479e8e47c4818de3dea1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e5aefffba53b4f80bead239161e5a276": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "da65aaaa04064d51acaa1b237fca78d1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "bafbc9b98ccb426eaa0ebade9a17907b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2ae31d8d5e544361a400e265376f667f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "628ff8e7a6ef4e5f8259bc6293230e82": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<center>\n",
        "<h1>\n",
        "<h1>APM 53674: ALTeGraD</h1>\n",
        "<h2>Lab Session 4: Distillation and Retrieval Augmented Generation</h2>\n",
        "<h4>Lecture: Dr. Guokan Shang<br>\n",
        "Lab: Yang Zhang and Xiao Fei</h4>\n",
        "<h5>Tuesday, November 18, 2025</h5>\n",
        "<br>\n",
        "</center>\n",
        "\n",
        "<hr style=\"border:10px solid gray\"> </hr>\n",
        "<p style=\"text-align: justify;\">\n",
        "This handout includes theoretical introductions, <font color='blue'>coding tasks</font> and <font color='red'>questions</font>. Before the deadline, you should submit <a href='https://forms.gle/9dyaes6dimfvyjwq6' target=\"_blank\">here</a> a <B>.ipynb</B> file named <b>Lastname_Firstname.ipynb</b> containing your notebook (with the gaps filled and your answers to the questions). Your answers should be well constructed and well justified. They should not repeat the question or generalities in the handout. When relevant, you are welcome to include figures, equations and tables derived from your own computations, theoretical proofs or qualitative explanations. One submission is required for each student. The deadline for this lab is <b>Novemver 23\n",
        ", 2025 11:59 PM</b>. No extension will be granted. Late policy is as follows: ]0, 24] hours late → -5 pts; ]24, 48] hours late → -10 pts; > 48 hours late → not graded (zero).\n",
        "</p>\n",
        "<hr style=\"border:5px solid gray\"> </hr>"
      ],
      "metadata": {
        "id": "fk4xRt_adQ0Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install Requirements"
      ],
      "metadata": {
        "id": "SMA5hf82PxnW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required dependencies\n",
        "!pip -q install torch tqdm jsonlines h5py\n",
        "!pip -q install --upgrade transformers accelerate vllm\n",
        "!pip install jedi\n",
        "# !pip -q install datasets==2.21.0 pandas==2.2.2\n",
        "# !pip -q install chromadb==0.4.22\n",
        "# !pip -q install \"numpy<2.0\" --force-reinstall"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3NTeIal8P0Tb",
        "outputId": "7e26c27b-a2cb-43b9-f92e-f3a2a9e9069b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.9/87.9 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m380.9/380.9 kB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m370.3/370.3 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m355.0/355.0 kB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.0/183.0 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m92.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.0/111.0 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.4/45.4 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m84.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m75.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.9/122.9 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m104.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.2/96.2 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m87.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m101.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.7/102.7 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m91.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m463.4/463.4 kB\u001b[0m \u001b[31m38.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m80.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.3/72.3 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m73.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m388.0/388.0 kB\u001b[0m \u001b[31m36.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m285.7/285.7 kB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.6/213.6 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m180.7/180.7 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m456.8/456.8 kB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m80.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.2/102.2 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.4/42.4 MB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m82.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.8/75.8 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.7/320.7 kB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m331.1/331.1 kB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m517.7/517.7 kB\u001b[0m \u001b[31m41.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m89.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m87.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m820.7/820.7 kB\u001b[0m \u001b[31m60.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m959.8/959.8 kB\u001b[0m \u001b[31m61.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\n",
            "gradio 5.49.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting jedi\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.12/dist-packages (from jedi) (0.8.5)\n",
            "Downloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jedi\n",
            "Successfully installed jedi-0.19.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1 - Model Distillation\n",
        "\n",
        "> In this section, you’ll learn the difference between **white-box** and **black-box** distillation, generate **synthetic data** to train a **student model**, and implement **white-box distillation** to specialize a model for a **RAG on Wikipedia** use case.\n"
      ],
      "metadata": {
        "id": "QRiJxB6PdljI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b><h4><font color='red'>\n",
        "\n",
        "<hr style=\"border:10px solid red\"> </hr>  \n",
        "Question 1:</br>\n",
        "Explain briefly the difference between black-box and white-box distillation? </br> What are the advantages and inconvenients of each approach?\n",
        "<hr style=\"border:10px solid red\"> </hr>  \n",
        "</font></h4>"
      ],
      "metadata": {
        "id": "07oSzYm_RsK9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b><h4><font color='green'>\n",
        "<hr style=\"border:10px solid green\"> </hr>\n",
        "Answer 1: </b><br>\n",
        "In white-box distillation, the teacher model is fully accessible: its architecture, parameters and internal representations (such as logits or hidden states) can be used to train the student. Since the student learns from internal signals rather than only from final predictions, the supervision is richer and typically leads to better performance with fewer training examples. However, this approach requires running the full teacher locally, which increases computational cost, and it is impossible when the model is proprietary.\n",
        "In black-box distillation, the teacher is treated as inaccessible and only its final outputs are observable, often through an API. The student learns to reproduce these outputs without access to the teacher's parameters or intermediate activations. This makes the method compatible with commercial LLMs, but the supervision is weaker and performance usually depends on generating a large amount of synthetic data to compensate for the lack of internal information.\n",
        "<hr style=\"border:10px solid green\"> </hr>\n",
        "</font></h4>\n",
        "\n"
      ],
      "metadata": {
        "id": "1EbhsdCNR3ME"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "RZvUA4sLTyKu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b><h4><font color='red'>\n",
        "\n",
        "<hr style=\"border:10px solid red\"> </hr>  \n",
        "Question 2:</br>\n",
        "What is the main requirement for a teacher/student pair of models to perform white-box distillation?\n",
        "<hr style=\"border:10px solid red\"> </hr>  \n",
        "</font></h4>\n",
        "\n"
      ],
      "metadata": {
        "id": "2n3yni_LSvp6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b><h4><font color='green'>\n",
        "<hr style=\"border:10px solid green\"> </hr>\n",
        "Answer 2: </b><br>\n",
        "The main requirement for white-box distillation is full access to the teacher's internal information: its architecture, parameters and intermediate representations. The student must be able to read the teacher's weights and hidden states, so distillation can align or imitate these internal features rather than only its final outputs.\n",
        "<hr style=\"border:10px solid green\"> </hr>\n",
        "</font></h4>\n"
      ],
      "metadata": {
        "id": "exgIPD7kS6nd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 - Synthetic Data Generation\n",
        "\n",
        "We're going to specialize a small 0.5B parameter model to perform RAG by distilling the abilities of a 7B parameter one.\n",
        "\n",
        "For that we'll be using `Qwen/Qwen2.5-0.5B-Instruct` as student and `Qwen/Qwen2.5-7B-Instruct-AWQ` (quantized version of `Qwen2.5-7B-Instruct`) as teacher.  \n",
        "\n",
        "In order to perform white-box distillation on generated answers we have two choices.\n",
        "\n",
        "1. We can perform a forward pass with the teacher, a forward pass with student on the complete sequence, and backprop difference of logprobs using KL Loss.\n",
        "2. Generation of samples with the teacher, save the logprobs and perform finetuning in a second step."
      ],
      "metadata": {
        "id": "gqGfvYFWeOnv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b><h4><font color='red'>\n",
        "\n",
        "<hr style=\"border:10px solid red\"> </hr>  \n",
        "Question 3:</br>\n",
        "What are the computational advantages of 1. vs 2.?\n",
        "<hr style=\"border:10px solid red\"> </hr>  \n",
        "</font></h4>"
      ],
      "metadata": {
        "id": "rBMX51LVXDpA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b><h4><font color='green'>\n",
        "<hr style=\"border:10px solid green\"> </hr>\n",
        "Answer 3: </b><br>\n",
        "In the first approach, the teacher and the student are run in a single pass: we compute the teacher's logits on the fly, immediately compute the student's logits on the same input, and directly backpropagate the KL loss. This avoids storing datasets of generated outputs and teacher log-probabilities, so no intermediate data has to be saved, serialized, or reloaded. As a result, it reduces memory and storage requirements and keeps training fully streamed.\n",
        "In the second approach, generation and training are separated. The teacher must first produce outputs and log-probabilities that must be saved and later reloaded for training. This two-step workflow increases storage usage, I/O cost, and preprocessing time, although it allows reusing the same dataset across multiple training runs.\n",
        "<hr style=\"border:10px solid green\"> </hr>\n",
        "</font></h4>"
      ],
      "metadata": {
        "id": "jL9i517XXIjx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We're going to generate a bunch of questions related to wikipedia paragraphs.\n",
        "For that we need to establish a system prompt that will allow for easy extraction."
      ],
      "metadata": {
        "id": "mtUANmlPZ92m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = \"\"\"\n",
        "You are a question generator.\n",
        "The user will provide:\n",
        "\n",
        "```json\n",
        "{\"title\": \"the title of an article\", \"paragraph\": \"a paragraph from that article\"}\n",
        "```\n",
        "\n",
        "Your task:\n",
        "\n",
        "* Generate one clear, self-contained question that can be answered using only the provided paragraph.\n",
        "* The question must be **specific**, **unambiguous**, and directly tied to the paragraph’s content.\n",
        "* Return the result with the question as a valid JSON** in the form:\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"question\": \"your question here\"\n",
        "}\n",
        "```\n",
        "\n",
        "Example:\n",
        "User input:\n",
        "\n",
        "```json\n",
        "{\n",
        "\"title\": \"The Moon Landing\",\n",
        "\"paragraph\": \"On July 20, 1969, Neil Armstrong became the first human to set foot on the Moon, followed by Buzz Aldrin.\"\n",
        "}\n",
        "```\n",
        "Assistant output:\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"question\": \"Who was the first human to set foot on the Moon during the Apollo 11 mission?\"\n",
        "}\n",
        "```\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "d5X6h9urZ8yQ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b><h4><font color='red'>\n",
        "\n",
        "<hr style=\"border:10px solid red\"> </hr>  \n",
        "Question 4:</br>\n",
        "In this system prompt, we don't generate answers, only questions. Explain why it's necessary in the context of white-box distillation.\n",
        "<hr style=\"border:10px solid red\"> </hr>  \n",
        "</font></h4>"
      ],
      "metadata": {
        "id": "XJnC4cjJaRpI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b><h4><font color='green'>\n",
        "<hr style=\"border:10px solid green\"> </hr>\n",
        "Answer 4: </b><br>\n",
        "In white-box distillation, the goal is to train the student model to imitate the internal behaviour of the teacher (logits, probabilities, hidden states) when producing an answer. If we allowed the teacher to answer as well as generate the questions, then the student would simply learn to imitate the *outputs* directly, drifting toward black-box distillation where only final responses are used. By forcing the prompt to generate only questions, we ensure that the downstream answers will be produced by the models themselves during the training step, so that the distillation loss is computed on their internal predictions. The student is therefore trained using the teacher's internal probabilities over the answer tokens, which is what characterizes white-box distillation.\n",
        "\n",
        "<hr style=\"border:10px solid green\"> </hr>\n",
        "</font></h4>"
      ],
      "metadata": {
        "id": "uo4p-YsrauUR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b><h4><font color='red'>\n",
        "\n",
        "<hr style=\"border:10px solid red\"> </hr>  \n",
        "Question 5:</br>\n",
        "For synthetic data generation we'll be using vLLM.\n",
        "vLLM is an optimized llm inference engine that can improve generation speed thanks to hardware specific optimization and computational tricks such as Prefix KV Caching.\n",
        "(https://docs.vllm.ai/en/latest/features/automatic_prefix_caching.html)</br></br> 1. Explain why prefix caching will be very efficient in our case? </br></br>\n",
        "2. What sampling `temperature` should we use? Justify.\n",
        "\n",
        "\n",
        "\n",
        "<hr style=\"border:10px solid red\"> </hr>  \n",
        "</font></h4>\n"
      ],
      "metadata": {
        "id": "m1WuONVpdVvv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b><h4><font color='green'>\n",
        "<hr style=\"border:10px solid green\"> </hr>\n",
        "Answer 5: </b><br>\n",
        "1.Why is prefix caching very efficient in our case?\n",
        "In our setting, the teacher will repeatedly generate answers for different synthetic examples, but all of these examples share the same **system prompt**, which remains identical. Prefix KV caching allows vLLM to compute the key - value attention cache for this fixed prefix only once, and then reuse it for every generation request. Since the system prompt is long and identical across all samples, avoiding its repeated forward computation greatly reduces inference time. Most of the computation then focuses only on the variable part (the generated question and answer), making prefix caching highly efficient.\n",
        "\n",
        "2.What sampling temperature should we use?\n",
        "For distillation, the goal is not to produce diverse or creative outputs, but to expose the student model to the teacher's typical reasoning patterns and token distribution. A low temperature (close to 0.0 - 0.3) reduces randomness and ensures that the generated answers are stable and representative of the teacher's preferred responses. This avoids noisy or highly variable outputs that would make the student learn unstable distributions. Using a near-deterministic temperature therefore produces consistent teacher behavior, leading to cleaner supervision for white-box distillation.\n",
        "\n",
        "<hr style=\"border:10px solid green\"> </hr>\n",
        "</font></h4>"
      ],
      "metadata": {
        "id": "XIazdZ4xd67Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b><h4><font color='blue'>\n",
        "<hr style=\"border:10px solid blue\"> </hr>\n",
        "Task 1: Optimized Generation of Synthetic Questions\n",
        "\n",
        "Complete the below code with the adequate options (prefix caching and `temperature`)\n",
        "\n",
        "<hr style=\"border:10px solid blue\"> </hr>\n",
        "</font></h4>\n"
      ],
      "metadata": {
        "id": "LxRUkWoOcAcW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from vllm import LLM\n",
        "from vllm import SamplingParams\n",
        "import os\n",
        "\n",
        "path_teacher = \"Qwen/Qwen2.5-7B-Instruct-AWQ\"\n",
        "\n",
        "llm = LLM(\n",
        "    model=path_teacher,\n",
        "    gpu_memory_utilization=0.9,\n",
        "    max_model_len=5000,\n",
        "    trust_remote_code=True        # required for Qwen models\n",
        ")\n",
        "\n",
        "sampling_params = SamplingParams(\n",
        "    temperature=0.1,              # low randomness for stable distillation targets\n",
        "    max_tokens=400\n",
        ")\n"
      ],
      "metadata": {
        "id": "IAfCQdVLayvA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 868,
          "referenced_widgets": [
            "5e01754b68774d39a032a446c28d4b7f",
            "c6f5ecd44927430eb2b3c3fc634813b5",
            "f07de4d28dc6489199a1268fa8ccf9db",
            "61b91a5a1a1d4f83af16a58d12ba68fe",
            "694edbddbdf14af3904d1b339041612e",
            "e9dc6a222560461daea7826b4cb23022",
            "a7fcf159596744879a8d6a461a909335",
            "a3ba64217d514de0ba32182797c4186f",
            "7fe70a9b21cf489d8e4c90c958a10ac6",
            "19be444790644ffbbe1fe9ad59444192",
            "cbffe446e191466fa64cb328906c0d68",
            "0de57ed1bda847a892fabdb65c528064",
            "d08e28b6b84c44878ffd956355453f21",
            "4372820dee824f888ea16cfb45c834fa",
            "5ab6fbf0ec1e48cab6b8112d33353b68",
            "15b7221fa0fb425c9c6dcb9940710878",
            "8655099ae10f40ce9093b256d67cde92",
            "422923bd82844a7c9dade9e89c8abca1",
            "9eec461994b544c6874bc8459c581380",
            "6b9c25526ade40ef9dd8b91f3ef7e966",
            "c7a1e081e7fe4a0bb1f1f2eaf096596f",
            "bd6cdeed57664ddd95116b5e5cf34f14",
            "74896ab5c01b4689a024532a16881b7a",
            "b63928852f744e0faca469155e4bd66b",
            "c25dd4f054fc42a98cf3f755df1b9545",
            "d5e2e1dc7c1d4b408f8fe421fd959064",
            "4a42d562805e483084b2e7ea2686fde6",
            "cc2701e1d20f4129b992597e4fd8e716",
            "961932df04c04dbea6a0d40a6d4d079e",
            "126bb8584b8f4b82914bc7858b2eb056",
            "ff5311cac39341f38b273f5bcb47cd0c",
            "4149cbc267804bf0864a05f982a3ff85",
            "9eecf82fbf4949caa22e72965805b14d",
            "f4fee765c40d4a5cbabd7dac51d2b2ee",
            "f783642b9e6a4626b03a33c2f74aad14",
            "fb5afdebcbc14c00b02f32e1f273dce5",
            "f5dd2fd8dd394e05be68d0c319fe5620",
            "8c654dc0d51745b49797ec390e9c469b",
            "7b6bca6b987247eab428f6f6045b94fc",
            "285494b48f524922b3f66ef79edb05b7",
            "4151b17dfcee40ac96e6ab141eecd83b",
            "c6d4b60ad433409fae97eb2b20513041",
            "476ac114ab384b379ffc4f68f6f6b114",
            "48954d618b9c45cfa13bd814ddd7f14b",
            "53b6c802bbb542eeb0957aa6eb74be87",
            "372221658ab44128b36985f4c43a550d",
            "82b488ce350e47589d5fe9a36758c75a",
            "3192a6cb3548403db2ce3edddf558abf",
            "f5909b04a4d045ad9639fd8bdc719ff4",
            "95c46ec5158d47b6a44592ac61f9006d",
            "3418e56a3ffd4a08ac1a3506f37aea66",
            "ba9810f8af5344bd896359810bcd5776",
            "b2a7efc9202d4673b1cd8efcd84ae393",
            "92b35b457e654b65bdcfa24949b953bd",
            "02c534b2971e47be84eff5582dc4b618",
            "1776ae04b1054a28bdbf127f0f5909ec",
            "b74e93cd86b14e1d94cae0a098eea926",
            "e59314c4fc764bff9254409515823ad2",
            "4c76703ed579486db071b66dced7b1b7",
            "2f43a2aaca3a4c0db072e2e960aad0f0",
            "0be140b01eba4cebb8a53b06812dfcdd",
            "c2805fdb6274462db996a369b690b782",
            "c3612350bb9b4ad0a276830d08e91e69",
            "1c29731c9b4641ae93065f0219e0912d",
            "081f31432b264747ba626f88e3484cce",
            "5cf27cb6a48e4a13ba6acdc191e56a10",
            "37ff59765e884f3a8f8e7a2f93f4500a",
            "c138619988cc48998bcafb833f7e794c",
            "dc66bb6d03b645e49b6cede613ed2cfc",
            "b5c4886221024758a8900e13fd5aac28",
            "f3042e9cce1047979a184588387edb21",
            "94c68d59112840829fd9249dd3740244",
            "cfde17b2cdf44ec49ffccfa46e0e14be",
            "c578dc3931c048578bf3406da39fdd0f",
            "563f635fa25542c496e4636d9e565bc3",
            "59236ba3fff3477ebe4b7cc0da030d86",
            "dbea4ec5d3454251bd5140899efc7da2",
            "56e7bfa86a5e47b1ae2b2121f57fd05b",
            "b633c6e31a7e45f9ae0957330d356224",
            "71103d7898d5416f855264696bbb1619",
            "db22159543114f5d8cb06e55c894c83a",
            "aca7d4b11dc0430fadee21c77351948a",
            "808c52b9b4454cacbafa1dfc4c2b7df6",
            "68da55a122304fcb844bc788bebcccd4",
            "11b825a9698c44a89811d5a929013de8",
            "901b5c93ce5745129f7803ef891f9bed",
            "82d342dffefa4532b75bb4208dd0c1e2",
            "2676b3eca4844717902cd3396775ec05"
          ]
        },
        "outputId": "dfdaea6a-7d09-4cc3-fb53-515d334b5ba5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO 11-23 15:06:04 [utils.py:253] non-default args: {'trust_remote_code': True, 'max_model_len': 5000, 'disable_log_stats': True, 'model': 'Qwen/Qwen2.5-7B-Instruct-AWQ'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/841 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5e01754b68774d39a032a446c28d4b7f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO 11-23 15:06:23 [model.py:631] Resolved architecture: Qwen2ForCausalLM\n",
            "INFO 11-23 15:06:23 [model.py:1745] Using max model len 5000\n",
            "INFO 11-23 15:06:26 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0de57ed1bda847a892fabdb65c528064"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Parse safetensors files:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "74896ab5c01b4689a024532a16881b7a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f4fee765c40d4a5cbabd7dac51d2b2ee"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "53b6c802bbb542eeb0957aa6eb74be87"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1776ae04b1054a28bdbf127f0f5909ec"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "37ff59765e884f3a8f8e7a2f93f4500a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/243 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "56e7bfa86a5e47b1ae2b2121f57fd05b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 11-23 15:06:29 [system_utils.py:103] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reasons: CUDA is initialized\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Engine core initialization failed. See root cause above. Failed core proc(s): {}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1436949855.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpath_teacher\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Qwen/Qwen2.5-7B-Instruct-AWQ\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m llm = LLM(\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath_teacher\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mgpu_memory_utilization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/llm.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, runner, convert, tokenizer, tokenizer_mode, skip_tokenizer_init, trust_remote_code, allowed_local_media_path, allowed_media_domains, tensor_parallel_size, dtype, quantization, revision, tokenizer_revision, seed, gpu_memory_utilization, swap_space, cpu_offload_gb, enforce_eager, disable_custom_all_reduce, hf_token, hf_overrides, mm_processor_kwargs, pooler_config, override_pooler_config, structured_outputs_config, kv_cache_memory_bytes, compilation_config, logits_processors, **kwargs)\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m         \u001b[0;31m# Create the Engine (autoselects V0 vs V1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 343\u001b[0;31m         self.llm_engine = LLMEngine.from_engine_args(\n\u001b[0m\u001b[1;32m    344\u001b[0m             \u001b[0mengine_args\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mengine_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0musage_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mUsageContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLLM_CLASS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/llm_engine.py\u001b[0m in \u001b[0;36mfrom_engine_args\u001b[0;34m(cls, engine_args, usage_context, stat_loggers, enable_multiprocessing)\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0;31m# Create the LLMEngine.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m         return cls(\n\u001b[0m\u001b[1;32m    175\u001b[0m             \u001b[0mvllm_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvllm_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m             \u001b[0mexecutor_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexecutor_class\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/llm_engine.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, vllm_config, executor_class, log_stats, aggregate_engine_logging, usage_context, stat_loggers, mm_registry, use_cached_outputs, multiprocess_mode)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0;31m# EngineCore (gets EngineCoreRequests and gives EngineCoreOutputs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m         self.engine_core = EngineCoreClient.make_client(\n\u001b[0m\u001b[1;32m    109\u001b[0m             \u001b[0mmultiprocess_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmultiprocess_mode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0masyncio_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core_client.py\u001b[0m in \u001b[0;36mmake_client\u001b[0;34m(multiprocess_mode, asyncio_mode, vllm_config, executor_class, log_stats)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmultiprocess_mode\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0masyncio_mode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mSyncMPClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvllm_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_stats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mInprocClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvllm_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_stats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core_client.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, vllm_config, executor_class, log_stats)\u001b[0m\n\u001b[1;32m    638\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvllm_config\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mVllmConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_class\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mExecutor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_stats\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m     ):\n\u001b[0;32m--> 640\u001b[0;31m         super().__init__(\n\u001b[0m\u001b[1;32m    641\u001b[0m             \u001b[0masyncio_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m             \u001b[0mvllm_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvllm_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core_client.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, asyncio_mode, vllm_config, executor_class, log_stats, client_addresses)\u001b[0m\n\u001b[1;32m    467\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m                 \u001b[0;31m# Engines are managed by this client.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 469\u001b[0;31m                 with launch_core_engines(vllm_config, executor_class, log_stats) as (\n\u001b[0m\u001b[1;32m    470\u001b[0m                     \u001b[0mengine_manager\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m                     \u001b[0mcoordinator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, typ, value, traceback)\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/utils.py\u001b[0m in \u001b[0;36mlaunch_core_engines\u001b[0;34m(vllm_config, executor_class, log_stats, num_api_servers)\u001b[0m\n\u001b[1;32m    905\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    906\u001b[0m         \u001b[0;31m# Now wait for engines to start.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 907\u001b[0;31m         wait_for_engine_startup(\n\u001b[0m\u001b[1;32m    908\u001b[0m             \u001b[0mhandshake_socket\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m             \u001b[0maddresses\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/utils.py\u001b[0m in \u001b[0;36mwait_for_engine_startup\u001b[0;34m(handshake_socket, addresses, core_engines, parallel_config, cache_config, proc_manager, coord_process)\u001b[0m\n\u001b[1;32m    962\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcoord_process\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcoord_process\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexitcode\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m                 \u001b[0mfinished\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcoord_process\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoord_process\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexitcode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 964\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m    965\u001b[0m                 \u001b[0;34m\"Engine core initialization failed. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m                 \u001b[0;34m\"See root cause above. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Engine core initialization failed. See root cause above. Failed core proc(s): {}"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b><h4><font color='blue'>\n",
        "<hr style=\"border:10px solid blue\"> </hr>\n",
        "Task 2: Question Generation</br>\n",
        "\n",
        "<hr style=\"border:10px solid blue\"> </hr>\n",
        "</font></h4>\n",
        "\n",
        "We're going to use the llm.chat vLLM api to generate our samples.\n",
        "Make the adequate code to generate the data and save it in a questions.jsonl file.\n",
        "\n",
        "Entries of jsonl file should look like:\n",
        "```json\n",
        "{\n",
        "  \"id_doc\": <wikipedia_article_id>,\n",
        "  \"id_paragraph\": <paragraph_dataset_id>,\n",
        "  \"question\": <generated_question>,\n",
        "  \"title\": <title_wikipedia_article>,\n",
        "  \"paragraph\": <text_of_paragraph>\n",
        "}\n",
        "```\n",
        "\n",
        "You should:\n",
        "\n",
        "1. Complete `def extract_question(generated_text: str) -> str:` to extract the generated question.\n",
        "2. Complete `def conversation_generator` to output a generator directly ingestible by `llm.chat` api\n",
        "3. Complete the dataloader and for loop to generate samples by batches of 4\n",
        "4. Save the generated questions in a `questions.jsonl` file"
      ],
      "metadata": {
        "id": "AiYVPVzEegzG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Iterable, Iterator, List\n",
        "import json\n",
        "\n",
        "def extract_question(generated_text: str) -> str:\n",
        "    \"\"\"Extract the question from the generated text. If the question is not\n",
        "    following the right format return None.\"\"\"\n",
        "    try:\n",
        "        j: dict = json.loads(generated_text)   # parse returned JSON\n",
        "        assert \"question\" in j\n",
        "        return j[\"question\"]\n",
        "    except Exception:  # includes wrong JSON format or missing key\n",
        "        return None\n",
        "\n",
        "\n",
        "def conversation_generator(\n",
        "    entries: Iterable[dict],\n",
        "    system_prompt: str\n",
        "    ) -> Iterator[dict]:\n",
        "\n",
        "    \"\"\"Generate the conversation with the model.\"\"\"\n",
        "    for entry in entries:\n",
        "        conversation: List[dict] = [\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": json.dumps(\n",
        "                    {\"title\": entry[\"title\"], \"paragraph\": entry[\"paragraph\"]}\n",
        "                )\n",
        "            }\n",
        "        ]\n",
        "        yield conversation"
      ],
      "metadata": {
        "id": "5-GzUDo9gxGB"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from datasets import load_dataset\n",
        "\n",
        "ds = load_dataset(\"EvanD/Lab4_wikiparagraphs\")\n",
        "\n",
        "batch_size = 4\n",
        "conversations = list(conversation_generator(ds[\"train\"], system_prompt))\n",
        "\n",
        "dataloader = DataLoader(conversations, batch_size=batch_size, shuffle=False, num_workers=2, prefetch_factor=2, collate_fn=lambda x: x)\n"
      ],
      "metadata": {
        "id": "ILcseIFmegZm"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.auto import tqdm\n",
        "import jsonlines\n",
        "\n",
        "\n",
        "with jsonlines.open(\"questions.jsonl\", \"w\") as writer:\n",
        "    for i, batch in tqdm(enumerate(dataloader)):\n",
        "        id_paragraph = i * batch_size\n",
        "        outputs = llm.chat(batch,\n",
        "                        sampling_params=sampling_params,\n",
        "                        use_tqdm=False)\n",
        "        for j, output in enumerate(outputs):\n",
        "          entry = {}\n",
        "          q = extract_question(output.outputs[0].text)\n",
        "\n",
        "          # You must verify that the output actually contains a question and write it\n",
        "          # To Complete\n",
        "          if q is not None:\n",
        "                doc = ds[\"train\"][id_paragraph + j]\n",
        "                entry = {\n",
        "                    \"id_doc\": doc[\"id_doc\"],\n",
        "                    \"id_paragraph\": doc[\"id_paragraph\"],\n",
        "                    \"question\": q,\n",
        "                    \"title\": doc[\"title\"],\n",
        "                    \"paragraph\": doc[\"paragraph\"]\n",
        "                }\n",
        "                writer.write(entry)"
      ],
      "metadata": {
        "id": "0ype0N4eedtJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246,
          "referenced_widgets": [
            "1024a6569b624a49bd522a8686fbd997",
            "45ef0d01cc0b467591eba9f721a85b9d",
            "c2a6037670c74dc4bc8f5acfecb6d02e",
            "c2916e9b515547cda2c98f435084a5cf",
            "a217bc7d74f842cda2724a2afd08902f",
            "ce1f07f45e974052be06808eaead3417",
            "417e6ab32e1644148a3dd18045c44b90",
            "867b0130c4d940d89c9d4a100f67ba25",
            "4a209a9026be4eaba5dbec31b7b98f02",
            "d896bd9ba1354c939fe45679cb5de811",
            "9138192c7f1c49d69d2b0fda83018e10"
          ]
        },
        "outputId": "88f97456-6969-4be8-b1e3-266be6035f6f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1024a6569b624a49bd522a8686fbd997"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'llm' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-716369475.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mid_paragraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         outputs = llm.chat(batch,\n\u001b[0m\u001b[1;32m      9\u001b[0m                         \u001b[0msampling_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msampling_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                         use_tqdm=False)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'llm' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 - Logprobs Generation\n",
        "\n",
        "We get to the second part of this distillation where we are interested in distilling the answers logprobs of our teacher model to specialize our 0.5B model to perform Retrieval Augmented Generation (RAG).\n",
        "\n",
        "First we're going to generate the logprobs with our 7B parameter model.\n",
        "\n",
        "We'll use the following system prompt:"
      ],
      "metadata": {
        "id": "kxDRanUZespX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = (\n",
        "    \"You are an assistant for a Retrieval-Augmented Generation (RAG) system.\\n\"\n",
        "    \"Answer the question using only the provided documents. \"\n",
        "    \"If the answer cannot be found in the provided documents, respond that the answer is not available in the provided document database. \"\n",
        "    \"Documents:\\n{context_block}\\n\\n\"\n",
        ")"
      ],
      "metadata": {
        "id": "XrSA_S5le2a5"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b><h4><font color='blue'>\n",
        "<hr style=\"border:10px solid blue\"> </hr>\n",
        "Task 3: Complete Conversation Generator</br>\n",
        "\n",
        "<hr style=\"border:10px solid blue\"> </hr>\n",
        "</font></h4>\n",
        "\n",
        "Based on the previous code you made to generate questions, make a second one to generate answers and saving logprobs. We updated the sampling parameters of vLLM to return the logprobs and the token ids of the 20 most probable tokens.\n",
        "\n",
        "As this can result in high quantity of data in practice we're going to store generated conversations in a .jsonl file and generated logprobs and token ids to a hdf5 file (see https://docs.h5py.org/en/stable/ for more information on hdf5).\n",
        "\n",
        "`save_logprobs_hdf5()` is already implemented for you, and allows to save the logprobs to a .h5 hdf5 file, and increments sequences automatically\n",
        "\n",
        "The structure of the conversation generator `conversation_generator()` function is implemented, you need to complete it.\n",
        "\n"
      ],
      "metadata": {
        "id": "QnYyOhFJv1qK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import h5py, os\n",
        "import numpy as np\n",
        "\n",
        "def save_logprobs_hdf5(path, sequences, start_idx=None):\n",
        "    \"\"\"\n",
        "    sequences: list of sequences\n",
        "       each sequence = list of steps\n",
        "          each step = {token_id: Logprob(logprob=..., ...), ...}\n",
        "    \"\"\"\n",
        "    mode = \"a\" if os.path.exists(path) else \"w\"\n",
        "    with h5py.File(path, mode) as f:\n",
        "        # choose index to start writing\n",
        "        if start_idx is None:\n",
        "            # auto-continue numbering if file already has data\n",
        "            existing = [int(k.split(\"_\")[1]) for k in f.keys() if k.startswith(\"seq_\")]\n",
        "            start_idx = max(existing)+1 if existing else 0\n",
        "\n",
        "        for s_i, seq in enumerate(sequences, start=start_idx):\n",
        "            g = f.create_group(f\"seq_{s_i}\")\n",
        "            for t_i, step in enumerate(seq):\n",
        "                token_ids = np.fromiter(step.keys(), dtype=np.int32)\n",
        "                logprobs  = np.array([lp.logprob for lp in step.values()], dtype=np.float32)\n",
        "                g.create_dataset(f\"step_{t_i}/token_ids\", data=token_ids, compression=\"gzip\")\n",
        "                g.create_dataset(f\"step_{t_i}/logprobs\",  data=logprobs,  compression=\"gzip\")\n",
        "\n"
      ],
      "metadata": {
        "id": "kFquvqqivjs-"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b><h4><font color='red'>\n",
        "\n",
        "<hr style=\"border:10px solid red\"> </hr>  \n",
        "Question 6:</br>\n",
        "When completing conversation_generator we have several ways of sampling.\n",
        "What are good paragraph sampling strategies we could use to ensure good performance of downstream model?\n",
        "\n",
        "<hr style=\"border:10px solid red\"> </hr>  \n",
        "</font></h4>\n"
      ],
      "metadata": {
        "id": "HxjgwhDhzzM-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b><h4><font color='green'>\n",
        "<hr style=\"border:10px solid green\"> </hr>\n",
        "Answer 6: </b><br>\n",
        "Good sampling should maximize the informational value of the generated questions. Rather than sampling paragraphs uniformly, we should bias our selection toward paragraphs that are neither too short nor too long (to avoid trivial or noisy supervision), contain richer content such as entities, dates or explanations, and maintain diversity across topics. This ensures the teacher produces varied and meaningful questions, improving the downstream student model.\n",
        "<hr style=\"border:10px solid green\"> </hr>\n",
        "</font></h4>"
      ],
      "metadata": {
        "id": "l7JIzEqU0Q91"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "doc_id_to_paragraphs = {}\n",
        "\n",
        "for line in ds[\"train\"]:\n",
        "    doc_id = line[\"id\"]\n",
        "    paragraph = line[\"paragraph\"]\n",
        "    title = line[\"title\"]\n",
        "    if doc_id not in doc_id_to_paragraphs:\n",
        "        doc_id_to_paragraphs[doc_id] = []\n",
        "    doc_id_to_paragraphs[doc_id].append(title + \" -- \" + paragraph)\n",
        "\n",
        "def conversation_generator(\n",
        "    path_jsonl: str,\n",
        "    system_prompt: str,\n",
        "    top_k: int = 3\n",
        "    ) -> Iterator[dict]:\n",
        "\n",
        "    \"\"\"Generate the conversation with the model.\"\"\"\n",
        "\n",
        "    with jsonlines.open(path_jsonl, \"r\") as f:\n",
        "      for q_p in f:\n",
        "        number_of_paragraphs_in_context = random.sample(range(1, top_k + 1), 1)[0]\n",
        "        paragraphs = [q_p[\"title\"] + \" -- \" + q_p[\"paragraph\"]]\n",
        "\n",
        "        while len(paragraphs) < number_of_paragraphs_in_context:\n",
        "          # With 50% probability, add a paragraph from the same article\n",
        "          if random.random() < 0.5:\n",
        "              same = random.choice(doc_id_to_paragraphs[q_p[\"id_doc\"]])\n",
        "              if same not in paragraphs:\n",
        "                  paragraphs.append(same)\n",
        "                  continue\n",
        "\n",
        "          # Add a paragraph from a different article\n",
        "          random_doc = random.choice(list(doc_id_to_paragraphs.keys()))\n",
        "          if random_doc != q_p[\"id_doc\"]:\n",
        "              other = random.choice(doc_id_to_paragraphs[random_doc])\n",
        "              if other not in paragraphs:\n",
        "                  paragraphs.append(other)\n",
        "                  continue\n",
        "\n",
        "        random.shuffle(paragraphs)\n",
        "        system_prompt_formatted = system_prompt.format(\n",
        "            context_block=\"\\n\\n\".join(f\"[Document {i+1}]: {doc}\" for i, doc in enumerate(paragraphs))\n",
        "        )\n",
        "        conversation = [\n",
        "            {\"role\": \"system\", \"content\": system_prompt_formatted},\n",
        "            {\"role\": \"user\", \"content\": q_p[\"question\"]}  # FIXED\n",
        "        ]\n",
        "        yield conversation\n"
      ],
      "metadata": {
        "id": "AX-wTajUzH-7"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from tqdm.auto import tqdm\n",
        "from typing import Iterable\n",
        "\n",
        "\n",
        "batch_size = 4\n",
        "\n",
        "conversations = list(conversation_generator(path_jsonl=\"questions.jsonl\", system_prompt=system_prompt)) # To Complete\n",
        "\n",
        "dataloader = DataLoader(conversations, batch_size=batch_size, shuffle=False, num_workers=2, prefetch_factor=2, collate_fn=lambda x: x)\n",
        "\n",
        "sampling_params = SamplingParams(temperature=0.2, max_tokens=400, logprobs=20)\n",
        "\n",
        "with jsonlines.open(\"conversations_rag.jsonl\", \"w\") as writer:\n",
        "    for batch in tqdm(dataloader):\n",
        "        out = llm.chat(batch,\n",
        "                        sampling_params=sampling_params,\n",
        "                        use_tqdm=False)\n",
        "        for i, b in enumerate(batch):\n",
        "            text = out[i].outputs[0].text\n",
        "            b.append({\"role\": \"assistant\", \"content\": text})\n",
        "            writer.write(b)\n",
        "        save_logprobs_hdf5(\"logprobs.h5\", [out[i].outputs[0].logprobs for i in range(len(out))])\n",
        "\n",
        "\n",
        "conversations = list(conversation_generator(path_jsonl=\"questions.jsonl\", system_prompt=system_prompt))\n"
      ],
      "metadata": {
        "id": "tUdhF7ZTz1ta",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "e2a4c17125b04ff68dd090e1e4941c4c",
            "25b32d47d9f640ab8a081ff522d10832",
            "b22088c27fb846a582161a2fec0fb079",
            "b58733d3bf80457a934832ee93ae4d6b",
            "4775c8ce43364700ae48ddf17d885a94",
            "5937f6cc637f479e8e47c4818de3dea1",
            "e5aefffba53b4f80bead239161e5a276",
            "da65aaaa04064d51acaa1b237fca78d1",
            "bafbc9b98ccb426eaa0ebade9a17907b",
            "2ae31d8d5e544361a400e265376f667f",
            "628ff8e7a6ef4e5f8259bc6293230e82"
          ]
        },
        "outputId": "29b832f2-5b06-40e6-c99b-2f186f0c4500"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e2a4c17125b04ff68dd090e1e4941c4c"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3 - KL-Divergence and Distillation\n",
        "\n",
        "**You should restart the notebook kernel to free the gpu memory from the 7B model that is no longer needed**\n",
        "\n",
        "Now that we have the generated conversations and their logprobs we can train our 0.5B model to output the same distribution.\n",
        "\n",
        "The attentive student would have noticed that we have an incomplete representation of the probability distribution over tokens due to only keeping the top-20 logprobs.\n"
      ],
      "metadata": {
        "id": "81G0b7cS01nX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b><h4><font color='red'>\n",
        "\n",
        "<hr style=\"border:10px solid red\"> </hr>  \n",
        "Question 7:</br>\n",
        "What are the two solutions you can see to approximate full distillation despite only having the top-20 logprobs?\n",
        "\n",
        "<hr style=\"border:10px solid red\"> </hr>  \n",
        "</font></h4>"
      ],
      "metadata": {
        "id": "TZQCifoJ4ml-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b><h4><font color='green'>\n",
        "<hr style=\"border:10px solid green\"> </hr>\n",
        "Answer 7: </b><br>\n",
        "Two reasonable approximations can be used when only the top-20 log-probabilities are available instead of the full vocabulary distribution:\n",
        "\n",
        "(1) Renormalization over the top-k tokens.\n",
        "We treat the retrieved top-20 probabilities as if they were the only candidates, exponentiate them, and renormalize so they sum to 1. This approximates the teacher distribution on a reduced support and allows a KL loss to be computed.\n",
        "\n",
        "(2) Add a “remaining mass” bucket.\n",
        "We compute the total missing probability mass (1 - sum of the top-20 probabilities) and assign it to one synthetic token representing all other unseen tokens. This keeps the distribution closer to the true one and penalizes the student for assigning probability outside the top-20.\n",
        "<hr style=\"border:10px solid green\"> </hr>\n",
        "</font></h4>"
      ],
      "metadata": {
        "id": "9WrDEzqJ43cG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We supply two functions to help in this implementation:\n",
        "\n",
        "- `find_subsequence()`that allows to find all the occurences of a token subsequence in a 1-D tensor\n",
        "- `get_labels()` that allows to expand the top-20 logprobs to the whole vocabulary, implicitly setting prob to zero for other tokens\n",
        "- `QwenKLDataset`that loads samples from .h5 and .jsonl files, remove problematic inconsistent tokenized examples and outputs samples tokenized for training.\n",
        "\n",
        "To simplify we will train with a batch size of 1, you can implement gradient accumulation if you wish.\n",
        "\n",
        "The `PREFIX_ASSISTANT`variable contains the tokens that encode for"
      ],
      "metadata": {
        "id": "I4W9x0NZ5OoN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-0.5B-Instruct\")"
      ],
      "metadata": {
        "id": "PLCK3P-59acl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def find_subsequence(input_ids: torch.Tensor, subseq: torch.Tensor):\n",
        "    \"\"\"Find the first index of each occurence of a subsequence in the input_ids.\"\"\"\n",
        "\n",
        "    subseq_len = len(subseq)\n",
        "    matches = []\n",
        "    for idx in range(input_ids.size(0) - subseq_len + 1):\n",
        "        if torch.equal(input_ids[idx:idx + subseq_len], subseq):\n",
        "            return [idx]\n",
        "    return []\n",
        "\n",
        "def get_labels(logprobs: torch.Tensor, tokens, size_vocab: int = 151936, offset: int = 0):\n",
        "    \"\"\"Get the greedy max probability tokenized sequence from the logprobs.\"\"\"\n",
        "    labels = torch.full((len(logprobs), size_vocab), torch.finfo(torch.float16).min, dtype=torch.float16)\n",
        "    for idx, logprobs_token in enumerate(logprobs):\n",
        "        for token_id, logprob in zip(tokens[idx], logprobs_token):\n",
        "            labels[idx][token_id] = logprob\n",
        "    return labels"
      ],
      "metadata": {
        "id": "fhqXsxRQ6jgr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "import jsonlines\n",
        "import h5py\n",
        "\n",
        "\n",
        "PREFIX_ASSISTANT = [198, 151644, 77091, 198]\n",
        "\n",
        "class QwenKLDataset(Dataset):\n",
        "    \"\"\"Dataset for finetuning Qwen model with KL divergence loss.\"\"\"\n",
        "    def __init__(\n",
        "            self,\n",
        "            path_h5,\n",
        "            path_jsonl\n",
        "        ):\n",
        "        self.entries = []\n",
        "        with jsonlines.open(path_jsonl, \"r\") as reader:\n",
        "            with h5py.File(path_h5, \"r\") as f:\n",
        "                for j, line in tqdm(enumerate(reader)):\n",
        "                    inputs = tokenizer.apply_chat_template(line, add_generation_prompt=False, tokenize=True, return_dict=True, return_tensors=\"pt\")\n",
        "                    idx_subseq = find_subsequence(inputs[\"input_ids\"][0], subseq = torch.tensor(PREFIX_ASSISTANT))[0]\n",
        "                    seq_f = [f[f\"seq_{j}\"][f\"step_{i}\"][\"token_ids\"][0] for i in range(len(f[f\"seq_{j}\"]))]\n",
        "                    try:\n",
        "                        assert len(inputs[\"input_ids\"][0][idx_subseq+3:-2]) == len(seq_f)\n",
        "                    except AssertionError:\n",
        "                        print(f\"AssertionError {j}, skipping inconsistent detokenization/tokenization\")\n",
        "                    tokens = [f[f\"seq_{j}\"][f\"step_{i}\"][\"token_ids\"][:] for i in range(len(f[f\"seq_{j}\"]))]\n",
        "                    logprobs = [f[f\"seq_{j}\"][f\"step_{i}\"][\"logprobs\"][:] for i in range(len(f[f\"seq_{j}\"]))]\n",
        "                    inputs[\"input_ids\"] = inputs[\"input_ids\"].cuda()\n",
        "                    self.entries.append(\n",
        "                        {\n",
        "                            \"inputs\": inputs,\n",
        "                            \"idx_subseq\": idx_subseq + 3,\n",
        "                            \"seq_len\": len(seq_f),\n",
        "                            \"logprobs\": torch.tensor(np.array(logprobs)),\n",
        "                            \"tokens\": torch.tensor(np.array(tokens))\n",
        "                        }\n",
        "                    )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.entries)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Return for a given entry:\n",
        "\n",
        "        new_tokens: torch.Tensor, the tokenized conversation with the logprobs of the assistant answer inserted.\n",
        "        idx_match: int, the index at which the assistant answer starts in the new_tokens.\n",
        "        len_logprob_sequence: int, the tokenized length of the assistant answer.\n",
        "        labels: torch.Tensor, the logprobs of the assistant answer for each token.\n",
        "        \"\"\"\n",
        "        entry = self.entries[idx]\n",
        "        entry[\"labels\"] = get_labels(entry[\"logprobs\"], entry[\"tokens\"], offset=0, size_vocab=151936).cuda()\n",
        "\n",
        "        return entry"
      ],
      "metadata": {
        "id": "b2B1QbHo6kd2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "KL-Divergence is a metric often used to quantify the difference of probability mass between two distributions.\n",
        "It's not mathematically defined as a distance because of its asymetric nature:\n",
        "\n",
        "$$D_{KL}(P \\parallel Q) = \\sum_{x} P(x) \\log \\frac{P(x)}{Q(x)}$$\n",
        "\n",
        "In knowledge distillation, we often minimize the **Kullback–Leibler divergence** between the teacher’s output distribution $P_T$ and the student’s output distribution $P_S$.\n",
        "\n",
        "$$\n",
        "D_{KL}(P_T \\parallel P_S) = \\sum_x P_T(x) \\log \\frac{P_T(x)}{P_S(x)}\n",
        "$$\n",
        "$$\n",
        "D_{KL}(P_S \\parallel P_T) = \\sum_x P_S(x) \\log \\frac{P_S(x)}{P_T(x)}\n",
        "$$\n",
        "\n",
        "\n",
        "<b><h4><font color='red'>\n",
        "\n",
        "<hr style=\"border:10px solid red\"> </hr>  \n",
        "Question 8:</br>\n",
        "What qualitative difference would it make if we minimize $D_{KL}(P_T \\parallel P_S)$ instead of $D_{KL}(P_S \\parallel P_T)$? How would it affect the convergence of the student distribution?\n",
        "\n",
        "<hr style=\"border:10px solid red\"> </hr>  \n",
        "</font></h4>\n",
        "\n"
      ],
      "metadata": {
        "id": "PaREIiPaENzm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b><h4><font color='green'>\n",
        "<hr style=\"border:10px solid green\"> </hr>\n",
        "Answer 8: </b><br>\n",
        "Minimizing DKL(PT | PS) makes the student try to cover all outcomes the teacher considers likely. It penalizes the student mainly when it assigns too little probability to tokens the teacher prefers, which encourages a broader distribution.\n",
        "Minimizing DKL(PS | PT) instead pushes the student to focus only on the teacher's most probable tokens and ignore alternatives. The student becomes more concentrated and mode-seeking, converging faster but losing diversity compared to the teacher.\n",
        "<hr style=\"border:10px solid green\"> </hr>\n",
        "</font></h4>"
      ],
      "metadata": {
        "id": "cV18ExQdISvW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**You need to ensure you restarted the kernel and have sufficiently free memory (no 7B model running)**\n",
        "\n",
        "check with `! nvidia-smi`"
      ],
      "metadata": {
        "id": "h0Sv8L5vLIYm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! nvidia-smi"
      ],
      "metadata": {
        "id": "PtU5tNhOLMK2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "This code is very basic, and allows you to test a training over the small number of samples you've generated. Due to limitations of Google Colab we cannot go much further, but this first part should have given you the basics on how to perform white-box distillation.\n",
        "\n",
        "<b><h4><font color='blue'>\n",
        "<hr style=\"border:10px solid blue\"> </hr>\n",
        "Task 4: Complete The Training Code</br>\n",
        "\n",
        "<hr style=\"border:10px solid blue\"> </hr>\n",
        "</font></h4>\n",
        "\n",
        "\n",
        "- Loss function\n",
        "- gradient accumulation handling\n",
        "\n"
      ],
      "metadata": {
        "id": "cxzgt4p5IzYX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from tqdm.auto import tqdm\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "student_model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2.5-0.5B-Instruct\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-0.5B-Instruct\")\n",
        "\n",
        "dataset = QwenKLDataset(path_h5=\"logprobs.h5\", path_json=\"questions.jsonl\")\n",
        "\n",
        "num_epochs = 2\n",
        "grad_accum_steps = 8\n",
        "optimizer = torch.optim.AdamW(student_model.parameters(), lr=5e-5)\n",
        "loader = DataLoader(dataset, batch_size=1, shuffle=True, collate_fn=lambda x: x[0])\n",
        "num_training_steps = num_epochs * len(loader) // grad_accum_steps\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, 0, num_training_steps)\n",
        "loss_fn = torch.nn.KLDivLoss(reduction=\"batchmean\")\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for i, batch in enumerate(loader):\n",
        "        outputs = student_model(batch[\"inputs\"][\"input_ids\"])\n",
        "        logits_student = outputs.logits[:, :-1, :]                     # align with next token\n",
        "        log_probs_student = torch.log_softmax(logits_student, dim=-1)\n",
        "\n",
        "        # teacher distribution already stored (normalized or renormalizable)\n",
        "        teacher_target = batch[\"teacher_probs\"][:, 1:, :]              # same alignment\n",
        "\n",
        "        batch_loss = loss_fn(log_probs_student, teacher_target)\n",
        "\n",
        "        batch_loss.backward()\n",
        "        # To Complete\n",
        "        if (i + 1) % grad_accum_steps == 0 or (i + 1) == len(loader):\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            optimizer.zero_grad()\n",
        "            print (f\"Epoch {epoch+1}, step {i+1}/{len(loader)}: loss = {batch_loss.item():.4f}\")\n",
        "    print(f\"Epoch {epoch+1}: loss = {outputs.loss.item():.4f}\")\n",
        "\n",
        "student_model.save_pretrained(\"finetuned_model\")\n",
        "tokenizer.save_pretrained(\"finetuned_model\")\n"
      ],
      "metadata": {
        "id": "N4c-zOROIZ8u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2 - Retrieval Augmented Generation (RAG)\n",
        "\n",
        "In this section, we will discuss the concept of **Retrieval-Augmented Generation (RAG)** — a framework that combines **information retrieval** and **language generation**. RAG enables language models to access **external knowledge sources** at inference time, reducing hallucinations and improving factual accuracy.\n",
        "\n",
        "We will explore how to:\n",
        "- Build and index a **Vector database** from a corpus (here: Wikipedia sample).\n",
        "- Retrieve the most relevant documents given a query using **embedding-based similarity**.\n",
        "- Integrate retrieval results into the **generation pipeline** to produce context-aware answers.\n"
      ],
      "metadata": {
        "id": "uECS-Qh7dv28"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# restart the session and run\n",
        "!pip -q install chromadb==0.4.22\n",
        "!pip -q install \"numpy<2.0\" --force-reinstall\n",
        "!pip -q install datasets==2.21.0 pandas==2.2.2"
      ],
      "metadata": {
        "id": "qwkhRwYVyYf-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d8f02cb-a2f2-47a2-ea13-4b7defa93aae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m509.0/509.0 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m44.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.3/103.3 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m72.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m208.0/208.0 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.9/65.9 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.3/132.3 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.3/137.3 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m73.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m517.7/517.7 kB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.4/128.4 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m60.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m456.8/456.8 kB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for chroma-hnswlib (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m114.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m527.3/527.3 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.6/177.6 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2024.6.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import random\n",
        "import torch\n",
        "import pandas as pd\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "from datasets import load_dataset\n",
        "from IPython.display import display, HTML\n",
        "from transformers import AutoTokenizer, AutoModel"
      ],
      "metadata": {
        "id": "lwhx5lJ2TQxh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "outputId": "2914afce-6e25-4bbc-98b2-115e3f1272b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4127232165.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m     ) from _err\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m from pandas._config import (\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0mget_option\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mset_option\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/_config/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;34m\"warn_copy_on_write\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m ]\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdates\u001b[0m  \u001b[0;31m# pyright: ignore[reportUnusedImport]  # noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m from pandas._config.config import (\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/_config/config.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m from pandas._typing import (\n\u001b[0m\u001b[1;32m     69\u001b[0m     \u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/_typing.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGenerator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBitGenerator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRandomState\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__dir__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0mpublic_symbols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'testing'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         public_symbols -= {\n\u001b[1;32m    339\u001b[0m             \u001b[0;34m\"core\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"matrixlib\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/random/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;31m# add these for module-freeze analysis (like PyInstaller)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_pickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_common\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_bounded_integers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/random/_pickle.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmtrand\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomState\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_philox\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPhilox\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_pcg64\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPCG64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPCG64DXSM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_sfc64\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSFC64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mnumpy/random/mtrand.pyx\u001b[0m in \u001b[0;36minit numpy.random.mtrand\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Configuration ---\n",
        "SEED = 42\n",
        "NUM_ROWS = 1000\n",
        "DATA_PATH = \"wikipedia_20231101_en_1000.csv\"\n",
        "\n",
        "if os.path.exists(DATA_PATH):\n",
        "    print(f\"✅ Found existing dataset at {DATA_PATH}\")\n",
        "    df = pd.read_csv(DATA_PATH)\n",
        "else:\n",
        "    print(\"⏳ Generating new dataset from Wikimedia (English, 2023-11-01)...\")\n",
        "    random.seed(SEED)\n",
        "\n",
        "    # Load the Wikipedia dataset\n",
        "    stream_ds = load_dataset(\n",
        "        \"wikimedia/wikipedia\",\n",
        "        \"20231101.en\",\n",
        "        split=\"train\",\n",
        "        streaming=True\n",
        "    )\n",
        "\n",
        "    buffered_stream = stream_ds.shuffle(seed=SEED, buffer_size=200_000)\n",
        "\n",
        "    sampled = []\n",
        "    for ex in buffered_stream:\n",
        "        try:\n",
        "            if int(ex[\"id\"]) % 2 == 0:\n",
        "                sampled.append(ex)\n",
        "            if len(sampled) >= NUM_ROWS:\n",
        "                break\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "    # Create DataFrame\n",
        "    df = pd.DataFrame(sampled)[[\"id\", \"url\", \"title\", \"text\"]]\n",
        "    df.to_csv(DATA_PATH, index=False)\n",
        "    print(f\"💾 Dataset saved to {DATA_PATH}\")\n"
      ],
      "metadata": {
        "id": "iRW9L9s4dbeT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Display Basic Info ---\n",
        "print(\"Sampled shape:\", df.shape)\n",
        "print(\"Columns:\", df.columns.tolist())\n",
        "display(df.head())"
      ],
      "metadata": {
        "id": "kfqExXsQaxGR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize Random Wikipedia Articles ---\n",
        "\n",
        "NUM_EXAMPLES = 3  # number of random samples to show\n",
        "samples = df.sample(NUM_EXAMPLES, random_state=random.randint(0, 10000))\n",
        "\n",
        "for _, row in samples.iterrows():\n",
        "    display(HTML(f\"\"\"\n",
        "    <hr style=\"border:2px solid #ccc\">\n",
        "    <h3><b>Title:</b> {row['title']}</h3>\n",
        "    <p><b>URL:</b> <a href=\"{row['url']}\" target=\"_blank\">{row['url']}</a></p>\n",
        "    <p style=\"text-align: justify;\"><b>Text:</b><br>{row['text']}</p>\n",
        "    \"\"\"))\n"
      ],
      "metadata": {
        "id": "foHQBZ7556Ss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Document Chunking**\n",
        "\n",
        "The first step in building a RAG pipeline is **chunking**, where large documents are divided into smaller, semantically coherent pieces.  \n",
        "Chunking allows the retriever to work on manageable text segments instead of entire documents, improving retrieval precision and reducing computational load.  \n"
      ],
      "metadata": {
        "id": "H0rj2jrugW1H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b><h4><font color='blue'>\n",
        "<hr style=\"border:10px solid blue\"> </hr>\n",
        "Task 1: Naive Fixed-Length Chunking  \n",
        "Split each document into overlapping fixed-length chunks to prepare text for retrieval.\n",
        "<hr style=\"border:10px solid blue\"> </hr>\n",
        "</font></h4>\n"
      ],
      "metadata": {
        "id": "Zxsosg3b99AH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def text_splitting(text, chunk_length=300, chunk_overlap=100):\n",
        "    \"\"\"\n",
        "    Splits text into fixed-length chunks with overlap.\n",
        "    \"\"\"\n",
        "    out = []\n",
        "    stride = chunk_length - chunk_overlap## FILL THE GAP: define the stride as the effective step between chunks\n",
        "    for i in range(0, len(text), stride):\n",
        "        chunk = text[i:i + chunk_length] ## FILL THE GAP: extract a substring of size 'chunk_length' starting at 'i'\n",
        "        out.append(chunk)\n",
        "    return out\n",
        "\n",
        "# Apply to all documents\n",
        "df[\"naive_chunks\"] = df[\"text\"].apply(lambda t: text_splitting(t, chunk_length=300, chunk_overlap=100))\n",
        "\n",
        "num_chunks = df[\"naive_chunks\"].apply(len)\n",
        "print(f\"Average number of chunks per document: {num_chunks.mean():.2f}\")\n",
        "print(f\"Total number of chunks: {num_chunks.sum()}\")\n",
        "\n",
        "example_idx = 0\n",
        "print(\"\\n--- Example document ---\")\n",
        "print(\"Title:\", df.iloc[example_idx][\"title\"])\n",
        "print(\"Original length:\", len(df.iloc[example_idx][\"text\"]))\n",
        "print(\"Number of chunks:\", len(df.iloc[example_idx][\"naive_chunks\"]))\n",
        "print(\"\\nFirst 2 chunks:\\n\")\n",
        "for i, c in enumerate(df.iloc[example_idx][\"naive_chunks\"][:2]):\n",
        "    print(f\"Chunk {i+1}:\\n{c[:400]}\\n{'-'*80}\")"
      ],
      "metadata": {
        "id": "CK5jseiB1n6C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b><h4><font color='blue'>\n",
        "<hr style=\"border:10px solid blue\"> </hr>\n",
        "Task 2: Paragraph-Aware Chunking  \n",
        "Implement a smarter chunking strategy by using the ('.') as a boundary to split text into sentences or short paragraphs, then regroup them until reaching the desired chunk length.\n",
        "<hr style=\"border:10px solid blue\"> </hr>\n",
        "</font></h4>\n"
      ],
      "metadata": {
        "id": "ntVGPW2x-sBl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def text_splitting_paragraph(text, chunk_length=300):\n",
        "    \"\"\"\n",
        "    Splits text by sentences/paragraphs (using '.' as boundary)\n",
        "    and groups them until reaching the desired chunk length.\n",
        "    \"\"\"\n",
        "    out = []\n",
        "    paragraph_list = text.split('.') ## FILL THE GAP: split the text into smaller parts using '.' as a separator\n",
        "    current_text = \"\"\n",
        "    length = 0\n",
        "    for par in paragraph_list:\n",
        "        if length != 0 and length + len(par) < chunk_length:\n",
        "            current_text += \" \" + par ## FILL THE GAP: extend the ongoing chunk with the next segment\n",
        "            length += len(par) ## FILL THE GAP: increment the total length accordingly\n",
        "        else:\n",
        "            if len(current_text) != 0:\n",
        "                out.append(current_text) ## FILL THE GAP: store the current completed chunk before starting a new one)\n",
        "            current_text = par ## FILL THE GAP: initialize a new chunk with the current paragraph\n",
        "            length = len(par) ## FILL THE GAP: reset the chunk length counter\n",
        "    if len(current_text) > 0:\n",
        "        out.append(current_text) ## FILL THE GAP: add the last remaining chunk to the list)\n",
        "    return out\n",
        "\n",
        "\n",
        "# Apply to all documents\n",
        "df[\"paragraph_chunks\"] = df[\"text\"].apply(lambda t: text_splitting_paragraph(t, chunk_length=300))\n",
        "\n",
        "# Compute stats\n",
        "num_chunks_par = df[\"paragraph_chunks\"].apply(len)\n",
        "print(f\"Average number of paragraph-based chunks per document: {num_chunks_par.mean():.2f}\")\n",
        "print(f\"Total number of paragraph-based chunks: {num_chunks_par.sum()}\")\n",
        "\n",
        "# Example comparison\n",
        "example_idx = 432 #Check out other examples\n",
        "print(\"\\n--- Example document ---\")\n",
        "print(\"Title:\", df.iloc[example_idx]['title'])\n",
        "print(\"Original length:\", len(df.iloc[example_idx]['text']))\n",
        "print(f\"Character based chunks: {len(df.iloc[example_idx]['naive_chunks'])}\")\n",
        "print(f\"Paragraph based chunks: {len(df.iloc[example_idx]['paragraph_chunks'])}\")\n",
        "\n",
        "print(\"\\nParagraph chunk preview:\\n\")\n",
        "for i, c in enumerate(df.iloc[example_idx]['paragraph_chunks'][:6]):\n",
        "    print(f\"Chunk {i+1}:\\n{c[:400]}\\n{'-'*80}\")\n"
      ],
      "metadata": {
        "id": "EXKREfWx1n8G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b><h4><font color='red'>\n",
        "\n",
        "<hr style=\"border:10px solid red\"> </hr>  \n",
        "Question 1:  \n",
        "In the paragraph-aware chunking method above, we simply split Wikipedia text using the '.' delimiter to approximate sentence boundaries.  \n",
        "Discuss whether this is an effective strategy for creating meaningful chunks in a RAG system.\n",
        "Propose one or more improved chunking strategies that could better capture document structure — and you may include code snippets to justify or demonstrate your approach.  \n",
        "<hr style=\"border:10px solid red\"> </hr>  \n",
        "</font></h4>\n"
      ],
      "metadata": {
        "id": "XMtzpHHADf9x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b><h4><font color='green'>\n",
        "<hr style=\"border:10px solid green\"> </hr>\n",
        "Answer 1: </b><br>\n",
        "Splitting on “.” is a weak heuristic for chunking Wikipedia text. It often breaks sentences incorrectly (e.g., “U.S.”, “Dr.”, numbers, URLs) and ignores real document structure. This leads to incomplete chunks and a loss of context, which reduces retrieval quality in RAG.\n",
        "\n",
        "A better approach is to first detect proper sentence boundaries (e.g., using spaCy or NLTK) and then regroup sentences until a target length is reached. Even more robust is to respect paragraph boundaries and only subdivide large paragraphs when needed. For example:\n",
        "\n",
        "```python\n",
        "import nltk\n",
        "nltk.download(\"punkt\")\n",
        "\n",
        "def better_chunks(text, max_len=300):\n",
        "    sents = nltk.sent_tokenize(text)\n",
        "    out, cur = [], \"\"\n",
        "    for s in sents:\n",
        "        if len(cur) + len(s) < max_len:\n",
        "            cur += \" \" + s\n",
        "        else:\n",
        "            out.append(cur.strip())\n",
        "            cur = s\n",
        "    if cur:\n",
        "        out.append(cur.strip())\n",
        "    return out\n",
        "\n",
        "```\n",
        "This produces coherent, meaningful units that are more suitable for retrieval.\n",
        "<hr style=\"border:10px solid green\"> </hr>\n",
        "</font></h4>"
      ],
      "metadata": {
        "id": "gL03t4mRDktx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b><h4><font color='red'>\n",
        "<hr style=\"border:10px solid red\"> </hr>\n",
        "Question 2: Consider a scenario where you want to perform RAG on source code (e.g., Python files, Java classes) instead of natural language text. Would the chunking methods demonstrated above (character-based and sentence/paragraph based with boundaries) work effectively for code? Explain why or why not, and describe how you would approach chunking source code to maintain semantic coherence. What specific characteristics of code structure would you need to consider?\n",
        "\n",
        "<hr style=\"border:10px solid red\"> </hr>\n",
        "</font></h4>"
      ],
      "metadata": {
        "id": "HHpQ-hmuDslc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b><h4><font color='green'>\n",
        "<hr style=\"border:10px solid green\"> </hr>\n",
        "Answer 2: </b><br>\n",
        "The chunking methods above are designed for natural language and therefore work poorly on source code. Splitting on character length or on punctuation such as “.” has no semantic meaning in code, and would break elements like function definitions, control blocks, imports, or multi-line expressions. This would produce incomplete chunks that cannot be executed or meaningfully retrieved, and RAG would return fragments lacking context (e.g., half of a function).\n",
        "For code, chunking must follow syntactic and structural units. Meaningful boundaries include functions, classes, import blocks, docstrings, and comment blocks. A typical strategy is to parse the file using a language parser and group code by logical blocks or by lines belonging to the same node in the syntax tree. Large blocks can then be subdivided while retaining full scopes.\n",
        "Important characteristics to consider include indentation, brackets {}, scopes, dependencies (e.g., helper functions), and docstrings that describe behavior. Respecting these elements ensures that chunks correspond to complete, understandable units of co\n",
        "<hr style=\"border:10px solid green\"> </hr>\n",
        "</font></h4>"
      ],
      "metadata": {
        "id": "su69vspNDtfF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving chunks ---\n",
        "\n",
        "# Flatten chunks into a new DataFrame\n",
        "records = []\n",
        "for _, row in df.iterrows():\n",
        "    doc_id = row[\"id\"]\n",
        "    title = row[\"title\"]\n",
        "    url = row[\"url\"]\n",
        "    for i, chunk in enumerate(row[\"paragraph_chunks\"]):\n",
        "        records.append({\n",
        "            \"doc_id\": doc_id,\n",
        "            \"title\": title,\n",
        "            \"url\": url,\n",
        "            \"chunk_id\": f\"{doc_id}_chunk_{i}\",\n",
        "            \"chunk_text\": chunk.strip()\n",
        "        })\n",
        "\n",
        "# Create the flattened chunks DataFrame\n",
        "chunks_df = pd.DataFrame(records)\n",
        "print(f\"Total chunks: {len(chunks_df)}\")\n",
        "print(f\"Average chunk length: {chunks_df['chunk_text'].apply(len).mean():.2f} characters\\n\")\n",
        "\n",
        "# Show example\n",
        "print(\"Example rows:\")\n",
        "display(chunks_df.head())\n",
        "\n",
        "chunks_df.to_csv(\"wikipedia_chunks.csv\", index=False)\n",
        "print(\"Chunks saved to 'wikipedia_chunks.csv'\")\n",
        "print(f\"Total chunks: {len(chunks_df)}\")\n"
      ],
      "metadata": {
        "id": "G2kXurtV_Z45"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <b>Part II: Embedding</b>\n",
        "\n",
        "After chunking our documents, the next step is to convert text chunks into vector representations (embeddings). These embeddings capture the semantic meaning of the text in a high-dimensional space, allowing us to measure similarity between chunks and queries mathematically.\n",
        "\n",
        "We will use **sentence-transformers/all-MiniLM-L6-v2**, a compact and efficient embedding model that produces 384-dimensional embeddings for English text. This model offers a strong balance between performance and computational efficiency, making it well-suited for our RAG pipeline.\n"
      ],
      "metadata": {
        "id": "1u01gpv68UrU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b><h4><font color='blue'>\n",
        "<hr style=\"border:10px solid blue\"> </hr>\n",
        "Task 3: </b><br>\n",
        "Fill in the <code>embed()</code> function to encode text chunks and generate normalized embeddings using <code>sentence-transformers/all-MiniLM-L6-v2</code>.  \n",
        "Then, apply it to all documents in <code>chunks_df</code> and store the results.\n",
        "<hr style=\"border:10px solid blue\"> </hr>\n",
        "</font></h4>\n"
      ],
      "metadata": {
        "id": "MZYveYQWfK-5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 2.3 Embedding Generation ---\n",
        "# Load model\n",
        "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModel.from_pretrained(model_name)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = model.to(device)\n",
        "model.eval()\n",
        "\n",
        "print(f\"Loaded embedding model: {model_name}\")\n",
        "\n",
        "\n",
        "# --- Define embedding function ---\n",
        "def embed(text_list, doc_type=\"document\"):\n",
        "    \"\"\"\n",
        "    Encodes a list of texts and returns normalized embeddings.\n",
        "    \"\"\"\n",
        "    encoded = tokenizer(\n",
        "        [f\"search_{doc_type}: {t}\" for t in text_list],\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model(**encoded)                       # FILL THE GAP: forward pass\n",
        "        token_embeddings = output.last_hidden_state     # FILL THE GAP: last hidden state\n",
        "        pooled = token_embeddings.mean(dim=1)           # FILL THE GAP: mean over sequence\n",
        "        pooled = torch.nn.functional.normalize(pooled, p=2, dim=1)  # FILL: L2 normalization\n",
        "\n",
        "    return pooled.cpu()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7S2wJvk9bW6p",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        },
        "outputId": "9542e284-ec82-4205-a62c-20f100393242"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'AutoTokenizer' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1622185679.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Load model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"sentence-transformers/all-MiniLM-L6-v2\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'AutoTokenizer' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Test with one example ---\n",
        "sample_text = [\"Artificial intelligence is transforming the world.\"]\n",
        "sample_emb = embed(sample_text)\n",
        "print(f\"Sample embedding shape: {sample_emb.shape}\")\n",
        "\n",
        "# --- Apply to all chunks ---\n",
        "print(f\"\\nGenerating embeddings for {len(chunks_df)} chunks...\")\n",
        "\n",
        "emb_list = []\n",
        "for i in tqdm(range(0, len(chunks_df), 32)):\n",
        "    batch = chunks_df[\"chunk_text\"].iloc[i:i+32].tolist()\n",
        "    emb = embed(batch, doc_type=\"document\")\n",
        "    emb_list.append(emb)\n",
        "\n",
        "chunk_embeddings = torch.cat(emb_list, dim=0).numpy()\n",
        "chunks_df[\"embedding\"] = list(chunk_embeddings)\n",
        "\n",
        "print(\"\\nEmbeddings generated and added to DataFrame.\")\n",
        "print(chunks_df[[\"chunk_id\", \"title\", \"embedding\"]].head())"
      ],
      "metadata": {
        "id": "7xi2k9PQsbXk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b><h4><font color='red'>\n",
        "<hr style=\"border:10px solid red\"> </hr>\n",
        "Question 3: In most retrieval systems, embeddings are represented as fixed-size vectors. Can you think of a way to design embeddings that can flexibly adjust their size or level of detail while still preserving meaningful similarity between representations? How could such an approach benefit Retrieval-Augmented Generation (RAG) systems in practice, particularly for improving efficiency or adapting to different computational budgets?\n",
        "\n",
        "<hr style=\"border:10px solid red\"> </hr>\n",
        "<i>Hint:</i> You can find the answer in the paper <a href=\"https://arxiv.org/pdf/2205.13147\" target=\"_blank\">Matryoshka Representation Learning</a>.\n",
        "</font></h4>\n"
      ],
      "metadata": {
        "id": "AX7tYUGYGBTm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Embeddings do not need to be fixed-size vectors. In Matryoshka Representation Learning, a representation is learned so that any prefix of the embedding (e.g., first 64, 128, 256 dimensions) still preserves meaningful similarity. The vector is trained with multiple nested objectives so that smaller “sub-embeddings” approximate the same semantic space as the full one.\n",
        "\n",
        "This produces a scalable embedding where you can trade off speed vs. accuracy without recomputing features. In a RAG system, this means:\n",
        "\n",
        "  1) low-dimension embeddings can be used for fast, cheap retrieval on large corpora,\n",
        "\n",
        "  2) dimension embeddings can refine or rerank only the most promising results.\n",
        "\n",
        "Thus, a single model can adapt to different budgets (mobile vs. server inference) and do retrieval more efficiently by performing coarse search first, fine search only where needed."
      ],
      "metadata": {
        "id": "MikJrERDfRIe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Building a Simple Vector Database**\n",
        "\n",
        "After generating embeddings for all our document chunks, the next step is to **store** them in a structure that allows fast similarity search.  \n",
        "In this section, we will build a **simple in-memory vector database** using PyTorch tensors.  \n",
        "Each entry in the database will correspond to a text chunk and its embedding, enabling efficient retrieval based on vector similarity.\n"
      ],
      "metadata": {
        "id": "ctQ8gslMjKmR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b><h4><font color='blue'>\n",
        "<hr style=\"border:10px solid blue\"> </hr>\n",
        "Task 4: </b><br>\n",
        "Fill in the code to populate the database by computing embeddings for all text chunks in <code>chunks_df</code> using the <code>embed()</code> function.\n",
        "<hr style=\"border:10px solid blue\"> </hr>\n",
        "</font></h4>\n"
      ],
      "metadata": {
        "id": "T1WHRh10VYG2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def populate_database(chunks_df, batch_size=16):\n",
        "    \"\"\"\n",
        "    Populates a vector database from precomputed chunks_df.\n",
        "    \"\"\"\n",
        "    n_chunks = len(chunks_df)\n",
        "\n",
        "    sample_emb = embed([chunks_df[\"chunk_text\"].iloc[0]])        # compute sample embedding\n",
        "    output_dim = sample_emb.shape[1]                              # embedding dimension\n",
        "\n",
        "    vectorial_database = torch.zeros((n_chunks, output_dim))      # empty tensor to store embeddings\n",
        "    chunk_list = chunks_df[\"chunk_text\"].tolist()\n",
        "\n",
        "    print(f\"Populating vector database with {n_chunks} chunks...\")\n",
        "\n",
        "    n = 0\n",
        "    for i in range(0, n_chunks, batch_size):\n",
        "        batch = chunk_list[i:i + batch_size]                      # select batch of texts\n",
        "        embeddings = embed(batch)                                 # compute batch embeddings\n",
        "        vectorial_database[n:n + len(batch)] = embeddings         # store embeddings\n",
        "        n += len(batch)\n",
        "\n",
        "    return chunk_list, vectorial_database\n"
      ],
      "metadata": {
        "id": "3Va4H572IQ67"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Build Vector Database\n",
        "chunk_list, vectorial_database = populate_database(chunks_df)\n",
        "\n",
        "print(\"\\n✅ Vector database successfully built.\")\n",
        "print(f\"Total stored chunks: {len(chunk_list)}\")\n",
        "print(f\"Database tensor shape: {tuple(vectorial_database.shape)}\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "piSelCZ6IQ9W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Save vector databse\n",
        "os.makedirs(\"vector_db\", exist_ok=True)\n",
        "\n",
        "# Save tensor + chunk list\n",
        "torch.save(vectorial_database, \"vector_db/vectorial_database.pth\")\n",
        "\n",
        "with open(\"vector_db/chunk_list.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(chunk_list, f, indent=4, ensure_ascii=False)\n",
        "\n",
        "print(\"✅ Saved:\")\n",
        "print(\" - vector_db/vectorial_database.pth\")\n",
        "print(\" - vector_db/chunk_list.json\")"
      ],
      "metadata": {
        "id": "dE7hrQ95JMVM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the database\n",
        "vectorial_database = torch.load(\"vector_db/vectorial_database.pth\", map_location=device)\n",
        "vectorial_database.requires_grad_(False)\n",
        "\n",
        "with open(\"vector_db/chunk_list.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    chunk_list = json.load(f)\n",
        "\n",
        "print(f\"✅ Loaded {len(chunk_list)} chunks.\")\n",
        "print(f\"Database shape: {vectorial_database.shape}\\n\")\n",
        "\n",
        "# Inspect first few entries\n",
        "for i, embedding_vector in enumerate(vectorial_database[:5]):\n",
        "    print(f\"Vector {i} → {embedding_vector[:5]}\")\n",
        "    print(f\"Text snippet: {chunk_list[i][:300]}\\n\")\n"
      ],
      "metadata": {
        "id": "KfiCmzroIpu3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Defining Similarity Metrics for Retrieval**\n",
        "\n",
        "After populating our vector database with embeddings, the next step in a RAG pipeline is to define a *similarity metric* to measure how close two vectors are in the embedding space.  \n",
        "Common metrics include **dot product**, **L2 distance**, and **cosine similarity**.  \n",
        "\n",
        "In most retrieval systems, cosine similarity is preferred because it measures the *angle* between two vectors rather than their magnitude, allowing comparison based purely on semantic direction instead of scale.\n"
      ],
      "metadata": {
        "id": "M_YHXmlylFbn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b><h4><font color='blue'>\n",
        "<hr style=\"border:10px solid blue\"> </hr>\n",
        "Task 5: </b><br>\n",
        "Fill in the code to implement the <code>cosine_similarity()</code> function.  \n",
        "<hr style=\"border:10px solid blue\"> </hr>\n",
        "</font></h4>\n"
      ],
      "metadata": {
        "id": "idmh4G__lGTG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cosine_similarity(query_embeddings, doc_embeddings):\n",
        "    \"\"\"\n",
        "    Computes cosine similarity between query and document embeddings\n",
        "    using manual normalization.\n",
        "    \"\"\"\n",
        "    query_magnitudes = torch.norm(query_embeddings, dim=1, keepdim=True)      # vector norms\n",
        "    normalized_queries = query_embeddings / query_magnitudes                  # normalize\n",
        "\n",
        "    doc_magnitudes = torch.norm(doc_embeddings, dim=1, keepdim=True)          # vector norms\n",
        "    normalized_docs = doc_embeddings / doc_magnitudes                         # normalize\n",
        "\n",
        "    similarity_matrix = normalized_queries @ normalized_docs.T                # dot product\n",
        "\n",
        "    return similarity_matrix\n",
        "\n",
        "\n",
        "\n",
        "# --- Example test ---\n",
        "query_embeddings = embed([\n",
        "    \"What is t-SNE?\",\n",
        "    \"Who is Laurens van der Maaten?\"\n",
        "], \"query\")\n",
        "\n",
        "doc_embeddings = embed([\n",
        "    \"t-SNE is a dimensionality reduction algorithm created by Laurens van der Maaten.\"\n",
        "], \"document\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    sim_cos = cosine_similarity(query_embeddings, doc_embeddings)\n",
        "\n",
        "print(\"🔍 Example cosine similarity scores:\\n\", sim_cos)"
      ],
      "metadata": {
        "id": "IRK6PNQhK1LB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b><h4><font color='blue'>\n",
        "<hr style=\"border:10px solid blue\"> </hr>\n",
        "Task 6: </b><br>\n",
        "Fill in the code to complete the <code>retrieve()</code> function.  \n",
        "It should encode the input query using <code>embed()</code>, compute similarity with all vectors in <code>vectorial_database</code>, and return the top-<i>k</i> most similar text chunks.\n",
        "<hr style=\"border:10px solid blue\"> </hr>\n",
        "</font></h4>\n"
      ],
      "metadata": {
        "id": "PeIEzoebV8wd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve(query,\n",
        "             vectorial_database=vectorial_database,\n",
        "             chunk_list=chunk_list,\n",
        "             topk=5,\n",
        "             verbose=False):\n",
        "    \"\"\"\n",
        "    Retrieves top-k most similar chunks to a query using dot-product similarity.\n",
        "    \"\"\"\n",
        "    with torch.no_grad():\n",
        "        query_embedding = embed([query])                          # encode query\n",
        "        query_embedding = query_embedding.to(device)              # move to device\n",
        "        similarity_scores = query_embedding @ vectorial_database.T # compute similarity\n",
        "        topk_results = torch.topk(similarity_scores, k=topk, dim=1)\n",
        "\n",
        "        if verbose:\n",
        "            for score, idx in zip(topk_results.values[0], topk_results.indices[0]):\n",
        "                print(f\"\\nScore: {score:.4f}\")\n",
        "                print(f\"Chunk:\\n{chunk_list[idx][:500]}\\n{'-'*80}\")\n",
        "\n",
        "        retrieved_chunks = [chunk_list[idx] for idx in topk_results.indices[0]]  # select text\n",
        "        return \"\\n\\n\".join(retrieved_chunks)  # concatenate as one string\n",
        "\n"
      ],
      "metadata": {
        "id": "y4Wv_MD1K6IL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example query\n",
        "query = \"When was Luigi Boria born?\" #Try different queries based on the documents in the wikipedia dataset\n",
        "result = retrieve(query, topk=3, verbose=True)\n"
      ],
      "metadata": {
        "id": "HsqTAtYAK6LS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b><h4><font color='red'>\n",
        "<hr style=\"border:10px solid red\"> </hr> Question 4: There are retrieval methods like BM25 that rely on lexical overlap between the query and documents, and others based on dense embeddings that capture semantic similarity beyond exact word matches. Explain how these two approaches differ in how they represent and compare text. Then, discuss how a hybrid retrieval strategy combining both can overcome their respective limitations and improve retrieval performance in RAG systems. <hr style=\"border:10px solid red\"> </hr> </font></h4>"
      ],
      "metadata": {
        "id": "u8AR7jFOLcPA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b><h4><font color='green'>\n",
        "<hr style=\"border:10px solid green\"> </hr>\n",
        "Answer 4: </b><br>\n",
        "Your answer here.\n",
        "<hr style=\"border:10px solid green\"> </hr>\n",
        "</font></h4>\n"
      ],
      "metadata": {
        "id": "tTT638v7MeMn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In real-world RAG systems, instead of manually storing and comparing vectors, we rely on **vector databases** such as **ChromaDB**, which are optimized for efficient **similarity search**, **indexing**, and **retrieval** at scale.  \n",
        "\n",
        "These databases provide:\n",
        "- Fast nearest-neighbor search (e.g., using HNSW graphs)  \n",
        "- Persistent storage for millions of embeddings  \n",
        "- Built-in support for different similarity metrics (cosine, L2, inner product)  \n"
      ],
      "metadata": {
        "id": "Rpq8WQilG8LE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import chromadb\n",
        "from chromadb.config import Settings\n",
        "\n",
        "# --- Initialize ChromaDB client ---\n",
        "chroma_client = chromadb.Client(Settings(\n",
        "    anonymized_telemetry=False,\n",
        "    allow_reset=True\n",
        "))\n",
        "\n",
        "# Reset ensures a clean state\n",
        "chroma_client.reset()\n",
        "\n",
        "# --- Create a collection ---\n",
        "# You can choose the similarity metric: \"cosine\", \"l2\", or \"ip\" (inner product)\n",
        "collection_name = \"wikipedia_chunks\"\n",
        "collection = chroma_client.create_collection(\n",
        "    name=collection_name,\n",
        "    metadata={\"hnsw:space\": \"cosine\"}  # cosine similarity works best for normalized embeddings\n",
        ")\n",
        "\n",
        "print(f\"✅ Created collection: {collection_name}\")\n"
      ],
      "metadata": {
        "id": "digvIbGF8h6P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the data\n",
        "ids = chunks_df[\"chunk_id\"].tolist()\n",
        "embeddings = chunks_df[\"embedding\"].tolist()\n",
        "documents = chunks_df[\"chunk_text\"].tolist()\n",
        "\n",
        "# Ensure all embeddings are plain Python lists\n",
        "embeddings = [e.tolist() if hasattr(e, \"tolist\") else e for e in embeddings]\n",
        "\n",
        "# Add useful metadata for inspection\n",
        "metadatas = [\n",
        "    {\n",
        "        \"doc_id\": row[\"doc_id\"],\n",
        "        \"title\": row[\"title\"],\n",
        "        \"url\": row[\"url\"]\n",
        "    }\n",
        "    for _, row in chunks_df.iterrows()\n",
        "]\n",
        "\n",
        "# Add to the collection\n",
        "collection.add(\n",
        "    ids=ids,\n",
        "    embeddings=embeddings,\n",
        "    documents=documents,\n",
        "    metadatas=metadatas\n",
        ")\n",
        "\n",
        "print(f\"\\n✅ Added {collection.count()} chunks to the collection\")\n",
        "print(f\"Collection metadata: {collection.metadata}\")\n"
      ],
      "metadata": {
        "id": "c6y61_JSNA_5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode the query using our embed() function\n",
        "query = \"When was Luigi Boria born?\"\n",
        "query_embedding = embed([query], doc_type=\"query\")[0].tolist()  # get single vector as list\n",
        "\n",
        "# Query the collection\n",
        "results = collection.query(\n",
        "    query_embeddings=[query_embedding],\n",
        "    n_results=3 # number of retrieved results\n",
        ")\n",
        "\n",
        "# Display results\n",
        "print(f\"🔎 Query: {query}\\n\" + \"=\" * 80)\n",
        "for i in range(len(results[\"documents\"][0])):\n",
        "    print(f\"\\nResult {i+1}:\")\n",
        "    print(f\"Title: {results['metadatas'][0][i]['title']}\")\n",
        "    print(f\"Similarity score: {1 - results['distances'][0][i]:.4f}\")  # cosine distance → similarity\n",
        "    print(f\"Text: {results['documents'][0][i][:300]}...\")\n",
        "    print(\"-\" * 80)\n"
      ],
      "metadata": {
        "id": "CHysGgWTNIKm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b><h4><font color='red'>\n",
        "<hr style=\"border:10px solid red\"> </hr>\n",
        "Question 5: </b><br>\n",
        "ChromaDB and other vector databases often rely on Hierarchical Navigable Small World (HNSW) graphs to perform efficient approximate nearest neighbor search.  \n",
        "Explain how the HNSW algorithm organizes data to enable fast and accurate retrieval in high-dimensional spaces.  \n",
        "Why is this structure particularly effective for large-scale embedding collections compared to brute-force search?\n",
        "\n",
        "<hr style=\"border:10px solid red\"> </hr>\n",
        "<i>Reference:</i> <a href=\"https://arxiv.org/pdf/1603.09320\" target=\"_blank\">Efficient and Robust Approximate Nearest Neighbor Search using Hierarchical Navigable Small World Graphs</a>\n",
        "</font></h4>\n"
      ],
      "metadata": {
        "id": "n2GomUuzYFdR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b><h4><font color='green'>\n",
        "<hr style=\"border:10px solid green\"> </hr>\n",
        "Answer 5: </b><br>\n",
        "BM25 is a lexical method: it represents text using exact word occurrences and compares queries and documents based on how often those same words appear, weighted by their importance in the corpus. It does not understand meaning—two texts using different wording for the same idea will look unrelated. In contrast, dense embeddings represent sentences as continuous vectors capturing semantic similarity, so two passages can be close even if they share few or no words. However, dense models may fail on very specific facts where exact wording matters (names, dates, formulas).\n",
        "A hybrid strategy combines both signals: BM25 provides strong precision for exact matches, while embeddings recover semantically related content. Together they reduce false negatives (missed paraphrased answers) and false positives (irrelevant semantic matches), leading to more robust and accurate retrieval in RAG systems, especially for factual QA or long-form reasoning.\n",
        "<hr style=\"border:10px solid green\"> </hr>\n",
        "</font></h4>\n"
      ],
      "metadata": {
        "id": "G5hHNO3LYMSa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <b> Two-Stage Retrieval: Dense Retrieval + Reranker</b>\n",
        "\n",
        "In RAG, the first retrieval step often returns passages that are similar in meaning but not always the most relevant.  \n",
        "A **reranker** fixes this by re-evaluating the top retrieved chunks using a stronger model that jointly reads the query and each document to assign a more accurate relevance score.\n"
      ],
      "metadata": {
        "id": "CR9LpejHea5x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import CrossEncoder\n",
        "\n",
        "# Load reranker model\n",
        "RERANKER_MODEL_NAME = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"\n",
        "reranker = CrossEncoder(RERANKER_MODEL_NAME)\n",
        "\n",
        "def retrieve_with_reranker(query, collection, initial_k=5, final_k=3):\n",
        "    \"\"\"\n",
        "    Retrieves and reranks candidate documents for a given query.\n",
        "    Returns both the initial dense results and reranked results.\n",
        "    \"\"\"\n",
        "    query_embedding = embed([query], doc_type=\"query\")[0].tolist()\n",
        "    candidates = collection.query(query_embeddings=[query_embedding], n_results=initial_k)\n",
        "\n",
        "    docs = candidates[\"documents\"][0]\n",
        "    metas = candidates[\"metadatas\"][0]\n",
        "    dense_scores = [(1 - s) for s in candidates[\"distances\"][0]]\n",
        "\n",
        "    pairs = [(query, d) for d in docs]\n",
        "    ce_scores = reranker.predict(pairs)\n",
        "\n",
        "    reranked = [\n",
        "        {\n",
        "            \"title\": metas[i].get(\"title\", \"\"),\n",
        "            \"url\": metas[i].get(\"url\", \"\"),\n",
        "            \"text\": docs[i],\n",
        "            \"dense_score\": dense_scores[i],\n",
        "            \"rerank_score\": float(ce_scores[i]),\n",
        "        }\n",
        "        for i in range(len(docs))\n",
        "    ]\n",
        "    reranked.sort(key=lambda x: x[\"rerank_score\"], reverse=True)\n",
        "\n",
        "    return docs, metas, dense_scores, reranked[:final_k]"
      ],
      "metadata": {
        "id": "urSlPBdORkab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "query = \"Where did Luigi Boria study?\" #Try other queries too\n",
        "docs, metas, dense_scores, top_reranked = retrieve_with_reranker(\n",
        "    query=query,\n",
        "    collection=collection,\n",
        "    initial_k=5,\n",
        "    final_k=3\n",
        ")\n",
        "\n",
        "# --- Display initial dense retrieval ---\n",
        "print(f\"\\nInitial dense retrieval (Top 5):\\n\" + \"=\" * 80)\n",
        "for i, (d, s, m) in enumerate(zip(docs, dense_scores, metas), 1):\n",
        "    print(f\"{i}. {m.get('title', '')}  |  Dense similarity: {s:.4f}\")\n",
        "    print(f\"Text: {d[:300].replace('\\n', ' ')}\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "# --- Display top reranked results ---\n",
        "print(f\"\\nAfter Cross-Encoder Reranking (Top 3):\\n\" + \"=\" * 80)\n",
        "for i, item in enumerate(top_reranked, 1):\n",
        "    print(f\"{i}. {item['title']}\")\n",
        "    print(f\"Dense similarity: {item['dense_score']:.4f} | Reranker score: {item['rerank_score']:.4f}\")\n",
        "    print(f\"Text: {item['text'][:300].replace('\\n', ' ')}\")\n",
        "    print(\"-\" * 80)"
      ],
      "metadata": {
        "id": "A9eao4mXtXkH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Bi-Encoder vs Cross-Encoder Architecture](https://raw.githubusercontent.com/UKPLab/sentence-transformers/master/docs/img/Bi_vs_Cross-Encoder.png)\n",
        "\n",
        "<b><h4><font color='red'>\n",
        "<hr style=\"border:10px solid red\"> </hr>\n",
        "Question 6:  \n",
        "The figure above compares a Bi-Encoder and a Cross-Encoder architecture.  \n",
        "Rerankers such as <code>cross-encoder/ms-marco-MiniLM-L-6-v2</code> use the second approach, jointly encoding the query and document through a single transformer.  \n",
        "Why does this joint encoding typically yield higher retrieval precision, and why is it applied as a second-stage reranker instead of being used directly for large-scale retrieval?\n",
        "<hr style=\"border:10px solid red\"> </hr>\n",
        "</font></h4>\n"
      ],
      "metadata": {
        "id": "w_sPNBkW1tNL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b><h4><font color='green'>\n",
        "<hr style=\"border:10px solid green\"> </hr>\n",
        "Answer 6: </b><br>\n",
        "Joint encoding yields higher precision because the model sees the *query and document together* and can attend across them token-by-token. This allows it to judge whether the document truly answers the query, capturing fine-grained relationships such as exact entity matching, negation, or subtle relevance that a bi-encoder (with separate embeddings) cannot model. It therefore produces more accurate relevance scores.\n",
        "However, this joint encoding is expensive: each query-document pair must be passed through the transformer, so the cost grows linearly with the number of candidates being compared. It is therefore impractical to apply over millions of documents for first-stage retrieval. Instead, a bi-encoder efficiently retrieves a short candidate list, and the cross-encoder is applied afterward to rerank only the top few hundred or dozens, combining high recall from the first stage with high precision in the second.\n",
        "\n",
        "<hr style=\"border:10px solid green\"> </hr>\n",
        "</font></h4>\n"
      ],
      "metadata": {
        "id": "-xxephXI16n9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Integrating Retrieved Context into the LLM’s Prompt**\n",
        "\n",
        "Now that we can retrieve and rerank the most relevant document chunks,  \n",
        "we integrate them directly into the **language model’s prompt**.  \n",
        "This step allows the model to **ground its answer on factual context** rather than relying solely on internal knowledge —  \n",
        "thereby improving accuracy and reducing hallucinations.\n"
      ],
      "metadata": {
        "id": "fTDWX3E82gkc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "gen_model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
        "gen_tok = AutoTokenizer.from_pretrained(gen_model_name, trust_remote_code=True)\n",
        "gen_model = AutoModelForCausalLM.from_pretrained(\n",
        "    gen_model_name,\n",
        "    device_map=\"auto\",\n",
        "    dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "def generate(msg, max_new_tokens=128, temperature=0.2):\n",
        "    messages = [{\"role\": \"user\", \"content\": msg}]\n",
        "    inputs = gen_tok.apply_chat_template(\n",
        "        messages,\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(gen_model.device)\n",
        "\n",
        "    outputs = gen_model.generate(\n",
        "        inputs,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        temperature=temperature,\n",
        "        top_p=0.9,\n",
        "        do_sample=True,\n",
        "        eos_token_id=gen_tok.eos_token_id,\n",
        "        pad_token_id=gen_tok.pad_token_id if gen_tok.pad_token_id is not None else gen_tok.eos_token_id\n",
        "    )\n",
        "\n",
        "    return gen_tok.decode(outputs[0][inputs.shape[-1]:], skip_special_tokens=True).strip()\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# NO-RAG vs WITH RAG\n",
        "# ============================================================\n",
        "query = \"When was Luigi Boria born?\" #Try other queries\n",
        "\n",
        "print(\"ORIGINAL PROMPT\\n\" + \"=\" * 60)\n",
        "print(query)\n",
        "\n",
        "print(\"\\nANSWER WITHOUT RAG\\n\" + \"=\" * 60)\n",
        "print(generate(query))\n",
        "\n",
        "docs, metas, dense_scores, top_reranked = retrieve_with_reranker(\n",
        "    query=query,\n",
        "    collection=collection,\n",
        "    initial_k=5,\n",
        "    final_k=3\n",
        ")\n",
        "\n",
        "context = \"\\n\\n\".join(h[\"text\"] for h in top_reranked)[:1600]\n",
        "rag_prompt = (\n",
        "    f\"Use only the context to answer. If unknown, say you don't know.\\n\\n\"\n",
        "    f\"Context:\\n{context}\\n\\n\"\n",
        "    f\"Question: {query}\\nAnswer:\"\n",
        ")\n",
        "\n",
        "print(\"\\nAUGMENTED PROMPT\\n\" + \"=\" * 60)\n",
        "print(rag_prompt)\n",
        "\n",
        "print(\"\\nANSWER WITH RAG\\n\" + \"=\" * 60)\n",
        "print(generate(rag_prompt))\n"
      ],
      "metadata": {
        "id": "qqwTZr0vJZw9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b><h4><font color='red'>\n",
        "<hr style=\"border:10px solid red\"> </hr> Question 7: In the RAG prompt construction step, we simply concatenate the retrieved chunks before the question. Discuss potential issues with this naive approach, such as token limits, redundancy, or irrelevant context dilution. Then, explain how we could select, weight, or summarize the retrieved chunks before injecting them into the prompt to improve generation quality and efficiency. <hr style=\"border:10px solid red\"> </hr> </font></h4>"
      ],
      "metadata": {
        "id": "b0Rlc7lVNBoj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b><h4><font color='green'>\n",
        "<hr style=\"border:10px solid green\"> </hr>\n",
        "Answer 7: </b><br>\n",
        "Naively concatenating all retrieved chunks creates several problems. If many chunks are long, the prompt may exceed the model's token limit, forcing truncation and possibly removing useful context. Different chunks can repeat the same information, wasting space, and irrelevant passages may dilute important signals, making the model more likely to produce generic or incorrect answers.\n",
        "A better strategy is to process the retrieved context before adding it to the prompt. We can rank or weight chunks by relevance score and include only the top few. Redundant passages can be removed through similarity filtering, and very long chunks can be summarized to preserve key facts in fewer tokens. Another option is to merge chunks into a structured format (e.g., bulleted facts) instead of raw text. These approaches reduce noise and improve both efficiency and answer quality.\n",
        "\n",
        "<hr style=\"border:10px solid green\"> </hr>\n",
        "</font></h4>\n"
      ],
      "metadata": {
        "id": "TX3eiuf6NIbV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **To go further**\n",
        "\n",
        "- Experiment with other chunking methods (e.g., semantic or recursive chunking).  \n",
        "- Explore **LangChain** and **LlamaIndex** for building modular RAG pipelines.  \n",
        "- Try **hybrid retrieval** combining sparse (BM25) and dense embeddings.  \n",
        "- Explore more advanced RAG methods such as **RAG-Fusion**, **Self-RAG**, and **Active-RAG**.  \n",
        "- Experiment with **Matryoshka Representation Learning** for scalable embeddings.  \n",
        "- Try **fine-tuning rerankers** or **retrievers** for domain-specific data.  \n"
      ],
      "metadata": {
        "id": "2dqvyNj-oSe7"
      }
    }
  ]
}